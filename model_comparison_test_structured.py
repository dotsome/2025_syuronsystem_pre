#!/usr/bin/env python3
# ===============================================
#  ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒå®Ÿé¨“ã‚¹ã‚¯ãƒªãƒ—ãƒˆ (Structured Outputsç‰ˆ)
# ===============================================
import os
import json
import time
import logging
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Literal
from dotenv import load_dotenv
import openai
from pydantic import BaseModel

# ç’°å¢ƒå¤‰æ•°èª­ã¿è¾¼ã¿
load_dotenv()
client = openai.OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.FileHandler(f'model_comparison_structured_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# =================================================
#           Pydanticã‚¹ã‚­ãƒ¼ãƒå®šç¾©
# =================================================

class Relationship(BaseModel):
    """ç™»å ´äººç‰©é–“ã®é–¢ä¿‚"""
    source: str
    target: str
    relation_type: Literal["directed", "bidirectional", "dotted"]
    label: str
    group: str = ""

class CharacterGraph(BaseModel):
    """ç™»å ´äººç‰©é–¢ä¿‚å›³ã®æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿"""
    center_person: str
    relationships: List[Relationship]

# ç„¡åŠ¹ãªãƒãƒ¼ãƒ‰åã®ã‚»ãƒƒãƒˆ
INVALID_NODES = {
    'ä¸æ˜', 'ä¸»ä½“', 'å®¢ä½“', 'ã‚°ãƒ«ãƒ¼ãƒ—', 'é–¢ä¿‚ã‚¿ã‚¤ãƒ—', 'é–¢ä¿‚è©³ç´°',
    '?', 'ï¼Ÿ', 'None', 'none', 'null', 'NULL', ''
}

# =================================================
#           ãƒ†ã‚¹ãƒˆè¨­å®š
# =================================================

# å›ºå®šãƒ¢ãƒ‡ãƒ«ï¼ˆå¿…ãšGPT-5.1ã‚’ä½¿ç”¨ï¼‰
FIXED_MODELS = {
    "character_judgment": "gpt-5.1",
    "center_person": "gpt-5.1"
}

# ãƒ†ã‚¹ãƒˆå¯¾è±¡ãƒ¢ãƒ‡ãƒ«ï¼ˆStructured Outputså¯¾å¿œãƒ¢ãƒ‡ãƒ«ã®ã¿ï¼‰
TEST_MODELS = {
    "structured_mermaid": [
        "gpt-4o",
        "gpt-4o-mini"
    ],
    "answer_generation": [
        "gpt-4o",
        "gpt-4o-mini"
    ]
}

# ãƒ†ã‚¹ãƒˆç”¨ã®è³ªå•
TEST_QUESTIONS = [
    {
        "id": "Q1",
        "question": "ãƒŸãƒŠã£ã¦èª°ã ã£ã‘ï¼Ÿ",
        "type": "character_identification"
    },
    {
        "id": "Q2",
        "question": "ã‚¿ãƒ‹ã‚¢ã¨ã‚«ãƒŠãƒ‡ã®é–¢ä¿‚æ€§ã«ã¤ã„ã¦æ•™ãˆã¦",
        "type": "relationship"
    },
    {
        "id": "Q3",
        "question": "ãƒ¬ã‚¤ãƒ³ã¯ã‚¢ãƒªã‚ªã‚¹ã®ã“ã¨ãŒãªã‚“ã§å«Œã„ãªã®ï¼Ÿ",
        "type": "character_motivation"
    },
    {
        "id": "Q4",
        "question": "ã‚¿ãƒ‹ã‚¢ã¨ãƒªãƒ¼ãƒ³ã®é–¢ä¿‚æ€§ã«ã¤ã„ã¦æ•™ãˆã¦",
        "type": "relationship"
    }
]

# =================================================
#           ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°
# =================================================

def openai_chat_timed(model: str, messages: List[Dict], log_label: str = None, **kwargs) -> Dict[str, Any]:
    """OpenAI APIã‚’å‘¼ã³å‡ºã—ã€æ™‚é–“ã¨ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’è¨ˆæ¸¬"""
    start_time = time.time()

    try:
        response = client.chat.completions.create(
            model=model,
            messages=messages,
            **kwargs
        )
        elapsed = time.time() - start_time

        usage = response.usage
        tokens = {
            "prompt": usage.prompt_tokens if usage else 0,
            "completion": usage.completion_tokens if usage else 0,
            "total": usage.total_tokens if usage else 0,
            "cached": 0
        }

        if hasattr(usage, 'prompt_tokens_details') and hasattr(usage.prompt_tokens_details, 'cached_tokens'):
            tokens['cached'] = usage.prompt_tokens_details.cached_tokens

        content = response.choices[0].message.content if response.choices else ""

        log_msg = f"âœ“ {log_label or 'APIå‘¼ã³å‡ºã—'}: model={model}, time={elapsed:.2f}s, tokens={tokens['prompt']}â†’{tokens['completion']}"
        if tokens['cached'] > 0:
            log_msg += f" (cached: {tokens['cached']})"
        logger.info(log_msg)

        return {
            "response": response,
            "time": elapsed,
            "tokens": tokens,
            "content": content
        }

    except Exception as e:
        elapsed = time.time() - start_time
        logger.error(f"âŒ APIå‘¼ã³å‡ºã—å¤±æ•—: {e}")
        raise

# =================================================
#           Structured Outputsé–¢æ•°
# =================================================

def build_mermaid_from_structured(graph: CharacterGraph) -> str:
    """
    Structured Outputsã®CharacterGraphã‹ã‚‰Mermaidå›³ã‚’æ§‹ç¯‰

    å¾“æ¥ã®CSVå‡¦ç†ã§è¡Œã£ã¦ã„ãŸå·¥å¤«ã‚’ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã§é©ç”¨:
    - é‡è¤‡ã‚¨ãƒƒã‚¸ã®æ’é™¤ï¼ˆåŒã˜ãƒšã‚¢ãƒ»åŒã˜æ–¹å‘ã¯1ã¤ã¾ã§ï¼‰
    - ãƒ©ãƒ™ãƒ«æ–‡å­—æ•°åˆ¶é™ï¼ˆ5æ–‡å­—ä»¥å†…ï¼‰
    - ãƒãƒ¼ãƒ‰ã®ã‚½ãƒ¼ãƒˆï¼ˆä¸€è²«æ€§ï¼‰
    - ã‚°ãƒ«ãƒ¼ãƒ—åã®ã‚µãƒ‹ã‚¿ã‚¤ã‚º
    """
    import re
    lines = ["graph LR"]

    # ãƒãƒ¼ãƒ‰ã¨ã‚¨ãƒƒã‚¸ã‚’åé›†ï¼ˆé‡è¤‡æ’é™¤ä»˜ãï¼‰
    nodes = set()
    edges = []
    groups = {}
    edge_map = {}  # (src, dst)ã®ãƒšã‚¢ã‚’ã‚­ãƒ¼ã«ã—ã¦é‡è¤‡ãƒã‚§ãƒƒã‚¯

    for rel in graph.relationships:
        # INVALIDãƒã‚§ãƒƒã‚¯
        if rel.source in INVALID_NODES or rel.target in INVALID_NODES:
            continue

        if not rel.source or not rel.target:
            continue

        # åŒã˜ãƒšã‚¢ï¼ˆé †åºã‚ã‚Šï¼‰ã®é‡è¤‡ãƒã‚§ãƒƒã‚¯
        edge_key = (rel.source, rel.target)
        if edge_key in edge_map:
            # æ—¢ã«åŒã˜æ–¹å‘ã®é–¢ä¿‚ãŒã‚ã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
            continue

        # ãƒãƒ¼ãƒ‰ç™»éŒ²
        nodes.add(rel.source)
        nodes.add(rel.target)

        # ã‚°ãƒ«ãƒ¼ãƒ—æƒ…å ±
        if rel.group:
            if rel.group not in groups:
                groups[rel.group] = set()
            groups[rel.group].add(rel.source)
            groups[rel.group].add(rel.target)

        # ã‚¨ãƒƒã‚¸è¨˜éŒ²ï¼ˆãƒ©ãƒ™ãƒ«ã¯5æ–‡å­—åˆ¶é™ï¼‰
        edge_symbol = "-->"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
        if rel.relation_type == "bidirectional":
            edge_symbol = "<-->"
        elif rel.relation_type == "dotted":
            edge_symbol = "-.->."

        edges.append({
            "src": rel.source,
            "dst": rel.target,
            "symbol": edge_symbol,
            "label": rel.label[:5]  # 5æ–‡å­—åˆ¶é™
        })
        edge_map[edge_key] = True

    # ãƒãƒ¼ãƒ‰IDã®ç”Ÿæˆï¼ˆå®‰å…¨ãªè­˜åˆ¥å­ï¼‰
    def safe_id(name: str) -> str:
        return f'id_{abs(hash(name)) % 10000}'

    node_ids = {name: safe_id(name) for name in nodes}

    # ãƒãƒ¼ãƒ‰å®šç¾©ï¼ˆã‚½ãƒ¼ãƒˆæ¸ˆã¿ï¼‰
    for name in sorted(nodes):
        node_id = node_ids[name]
        lines.append(f'    {node_id}["{name}"]')

    # ã‚°ãƒ«ãƒ¼ãƒ—å®šç¾©ï¼ˆã‚°ãƒ«ãƒ¼ãƒ—åã‚’ã‚µãƒ‹ã‚¿ã‚¤ã‚ºï¼‰
    if groups:
        lines.append('')
        for group_name, group_nodes in groups.items():
            # ç‰¹æ®Šæ–‡å­—ã‚’é™¤å»ã—ã¦ã‚µãƒ‹ã‚¿ã‚¤ã‚º
            safe_group_name = re.sub(r'[^0-9A-Za-z_\u3040-\u30FF\u4E00-\u9FFF\s]', '', group_name)
            lines.append(f'    subgraph {safe_group_name}')
            for node in sorted(group_nodes):
                if node in node_ids:
                    lines.append(f'        {node_ids[node]}')
            lines.append('    end')

    # ã‚¨ãƒƒã‚¸å®šç¾©
    lines.append('')
    for edge in edges:
        if edge["src"] in node_ids and edge["dst"] in node_ids:
            src_id = node_ids[edge["src"]]
            dst_id = node_ids[edge["dst"]]

            if edge["label"]:
                if edge["symbol"] == "<-->":
                    lines.append(f'    {src_id} <-->|{edge["label"]}| {dst_id}')
                elif edge["symbol"] == "-.->":
                    lines.append(f'    {src_id} -.->|{edge["label"]}| {dst_id}')
                else:
                    lines.append(f'    {src_id} -->|{edge["label"]}| {dst_id}')
            else:
                lines.append(f'    {src_id} {edge["symbol"]} {dst_id}')

    # ä¸­å¿ƒäººç‰©ãƒã‚¤ãƒ©ã‚¤ãƒˆï¼ˆfuzzy matchingï¼‰
    if graph.center_person:
        if graph.center_person in node_ids:
            lines.append(f'\n    style {node_ids[graph.center_person]} fill:#FFD700,stroke:#FF8C00,stroke-width:4px')
        else:
            # éƒ¨åˆ†ä¸€è‡´ã§æ¤œç´¢
            for node_name in node_ids:
                if graph.center_person in node_name or node_name in graph.center_person:
                    lines.append(f'\n    style {node_ids[node_name]} fill:#FFD700,stroke:#FF8C00,stroke-width:4px')
                    break  # æœ€åˆã«ãƒãƒƒãƒã—ãŸãƒãƒ¼ãƒ‰ã®ã¿ã‚’ãƒã‚¤ãƒ©ã‚¤ãƒˆ

    return '\n'.join(lines)

def process_structured_mermaid(model: str, story_text: str, question: str,
                               center_person: str) -> Dict[str, Any]:
    """
    Structured Outputsã§ã‚°ãƒ©ãƒ•ç”Ÿæˆï¼ˆ1ã‚¹ãƒ†ãƒƒãƒ—ï¼‰
    """
    prompt = f"""
æœ¬æ–‡:
{story_text}

è³ªå•: {question}
ä¸­å¿ƒäººç‰©: {center_person}

ã‚¿ã‚¹ã‚¯: æœ¬æ–‡ã‚’èª­ã¿ã€{center_person}ã‚’ä¸­å¿ƒã¨ã—ãŸç™»å ´äººç‰©ã®é–¢ä¿‚å›³ã‚’æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚

ã€é‡è¦ãªæ³¨æ„äº‹é …ã€‘
âŒ çµ¶å¯¾ã«ã‚„ã£ã¦ã¯ã„ã‘ãªã„ã“ã¨:
- ã€Œä¸æ˜ã€ã€Œè³ªå•è€…ã€ã€Œä¸»ä½“ã€ã€Œå®¢ä½“ã€ãªã©ã®æŠ½è±¡çš„ãªäººç‰©åã¯ä½¿ç”¨ç¦æ­¢
- å®Ÿåœ¨ã—ãªã„äººç‰©ã‚’å«ã‚ãªã„

âœ… æ­£ã—ã„ä¾‹:
- center_person: "ãƒŸãƒŠ"
- relationships: [
    {{"source": "ãƒŸãƒŠ", "target": "ã‚¢ãƒªã‚ªã‚¹", "relation_type": "bidirectional", "label": "ä»²é–“", "group": "å‹‡è€…ãƒ‘ãƒ¼ãƒ†ã‚£ãƒ¼"}},
    {{"source": "ãƒŸãƒŠ", "target": "ãƒ¬ã‚¤ãƒ³", "relation_type": "bidirectional", "label": "å…ƒä»²é–“", "group": ""}}
  ]

è¦ä»¶:
1. {center_person}ã‚’å¿…ãšå«ã‚ã‚‹
2. å®Ÿåœ¨ã™ã‚‹ç™»å ´äººç‰©ã®ã¿ï¼ˆå…·ä½“çš„ãªäººç‰©åï¼‰
3. ä¸»è¦ãªé–¢ä¿‚ã®ã¿ï¼ˆ5-10äººç¨‹åº¦ï¼‰
4. é–¢ä¿‚ã‚¿ã‚¤ãƒ—:
   - directed: ä¸€æ–¹å‘ï¼ˆä¸Šå¸â†’éƒ¨ä¸‹ãªã©ï¼‰
   - bidirectional: åŒæ–¹å‘ï¼ˆå‹äººã€ä»²é–“ãªã©ï¼‰
   - dotted: è£œåŠ©çš„ãªé–¢ä¿‚
5. labelã¯ç°¡æ½”ã«ï¼ˆ5æ–‡å­—ä»¥å†…æ¨å¥¨ï¼‰
6. åŒã˜2äººã®é–“ã®é–¢ä¿‚ã¯æœ€å¤§2æœ¬ã¾ã§

**çµ¶å¯¾ã«å®ˆã‚‹ã“ã¨:**
- ã€Œä¸æ˜ã€ã€Œä¸»ä½“ã€ã€Œå®¢ä½“ã€ãªã©ã®æŠ½è±¡çš„ãªåå‰ã¯çµ¶å¯¾ã«ä½¿ç”¨ã—ãªã„
- å¿…ãšå®Ÿåœ¨ã™ã‚‹ç™»å ´äººç‰©ã®ã¿ã‚’ä½¿ç”¨ã™ã‚‹
- {center_person}è‡ªèº«ã‚’å¿…ãšå«ã‚ã‚‹
"""

    start_time = time.time()

    try:
        response = client.beta.chat.completions.parse(
            model=model,
            messages=[
                {"role": "system", "content": "ç™»å ´äººç‰©ã®é–¢ä¿‚å›³ã‚’æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã§å‡ºåŠ›ã—ã¾ã™ã€‚"},
                {"role": "user", "content": prompt}
            ],
            response_format=CharacterGraph,
            temperature=0.3
        )

        elapsed = time.time() - start_time

        usage = response.usage
        tokens = {
            "prompt": usage.prompt_tokens if usage else 0,
            "completion": usage.completion_tokens if usage else 0,
            "total": usage.total_tokens if usage else 0,
            "cached": 0
        }

        if hasattr(usage, 'prompt_tokens_details') and hasattr(usage.prompt_tokens_details, 'cached_tokens'):
            tokens['cached'] = usage.prompt_tokens_details.cached_tokens

        graph_data = response.choices[0].message.parsed
        mermaid_code = build_mermaid_from_structured(graph_data)

        logger.info(f"âœ“ Structured Mermaidç”Ÿæˆ({model}): time={elapsed:.2f}s, tokens={tokens['prompt']}â†’{tokens['completion']}, cached={tokens['cached']}, relationships={len(graph_data.relationships)}")

        return {
            "response": response,
            "time": elapsed,
            "tokens": tokens,
            "content": mermaid_code,
            "graph_data": graph_data
        }

    except Exception as e:
        elapsed = time.time() - start_time
        logger.error(f"âŒ Structured Mermaidç”Ÿæˆå¤±æ•—: {e}")
        raise

def process_center_person(story_text: str, question: str, character_summary: str) -> Dict[str, Any]:
    """ä¸­å¿ƒäººç‰©ç‰¹å®šãƒ—ãƒ­ã‚»ã‚¹ï¼ˆå›ºå®š: GPT-5.1ï¼‰"""
    prompt = f"""ç™»å ´äººç‰©æƒ…å ±:
{character_summary}

---

è³ªå•: {question}

ã“ã®è³ªå•ã®ä¸­å¿ƒã¨ãªã‚‹ç™»å ´äººç‰©ã®åå‰ã‚’1ã¤ã ã‘ç­”ãˆã¦ãã ã•ã„ã€‚

è¦ä»¶:
- ç™»å ´äººç‰©æƒ…å ±ã«è¨˜è¼‰ã•ã‚Œã¦ã„ã‚‹æ­£ç¢ºãªäººç‰©åã§å›ç­”
- äººç‰©åã®ã¿ã‚’1è¡Œã§å‡ºåŠ›ï¼ˆèª¬æ˜ä¸è¦ï¼‰

å›ç­”:"""

    messages = [
        {"role": "system", "content": "è³ªå•ã®ä¸­å¿ƒäººç‰©ã‚’ç‰¹å®šã—ã¾ã™ã€‚"},
        {"role": "user", "content": prompt}
    ]

    return openai_chat_timed(
        model=FIXED_MODELS["center_person"],
        messages=messages,
        log_label="ä¸­å¿ƒäººç‰©ç‰¹å®š",
        temperature=0.0
    )

def process_answer_generation(model: str, story_text: str, question: str,
                              mermaid_code: str) -> Dict[str, Any]:
    """å›ç­”ç”Ÿæˆãƒ—ãƒ­ã‚»ã‚¹"""
    prompt = f"""ä»¥ä¸‹ã®æœ¬æ–‡ã¨ç™»å ´äººç‰©é–¢ä¿‚å›³ã‚’å‚è€ƒã«ã€è³ªå•ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚

æœ¬æ–‡:
{story_text}

ç™»å ´äººç‰©é–¢ä¿‚å›³:
{mermaid_code}

è³ªå•: {question}

å›ç­”:"""

    messages = [
        {"role": "system", "content": "è³ªå•ã«å›ç­”ã™ã‚‹ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"},
        {"role": "user", "content": prompt}
    ]

    kwargs = {"log_label": f"å›ç­”ç”Ÿæˆ({model})", "temperature": 0.7}

    return openai_chat_timed(
        model=model,
        messages=messages,
        **kwargs
    )

# =================================================
#           ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
# =================================================

def run_benchmark():
    """Structured Outputsç‰ˆãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯"""
    logger.info("=" * 80)
    logger.info("ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒå®Ÿé¨“ (Structured Outputsç‰ˆ)")
    logger.info("=" * 80)

    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    with open('beast_text.json', 'r', encoding='utf-8') as f:
        story_data = json.load(f)

    with open('character_summary.txt', 'r', encoding='utf-8') as f:
        character_summary = f.read()

    story_text = "\n\n".join([
        f"ã€{sec['section']}ç« ã€‘ {sec['title']}\n\n{sec['text']}"
        for sec in story_data[:30]
    ])

    results = []
    output_dir = Path("mermaid_outputs_structured")
    output_dir.mkdir(exist_ok=True)

    # =================================================
    # Prompt Cacheã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ï¼ˆåˆå›é«˜é€ŸåŒ–ï¼‰
    # =================================================
    logger.info("ğŸ”¥ Prompt Cacheã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ä¸­...")
    try:
        # ãƒ€ãƒŸãƒ¼è³ªå•ã§ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä½œæˆï¼ˆå„ãƒ¢ãƒ‡ãƒ«ã”ã¨ï¼‰
        warmup_question = "ä¸»äººå…¬ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„"

        # ä¸­å¿ƒäººç‰©ç‰¹å®šã®ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—
        _ = process_center_person(story_text, warmup_question, character_summary)

        # Structured Mermaidç”Ÿæˆã®ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ï¼ˆå„ãƒ¢ãƒ‡ãƒ«ï¼‰
        for mermaid_model in TEST_MODELS["structured_mermaid"]:
            _ = process_structured_mermaid(
                mermaid_model,
                story_text,
                warmup_question,
                "ä¸»äººå…¬"
            )

        # å›ç­”ç”Ÿæˆã®ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ï¼ˆå„ãƒ¢ãƒ‡ãƒ«ï¼‰
        dummy_mermaid = "graph LR\n    A[ä¸»äººå…¬]"
        for answer_model in TEST_MODELS["answer_generation"]:
            _ = process_answer_generation(
                answer_model,
                story_text,
                warmup_question,
                dummy_mermaid
            )

        logger.info("âœ… Prompt Cacheã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—å®Œäº†")
        logger.info("")
    except Exception as e:
        logger.warning(f"âš ï¸ ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—å¤±æ•—ï¼ˆç¶šè¡Œã—ã¾ã™ï¼‰: {e}")
        logger.info("")

    # ãƒ†ã‚¹ãƒˆçµ„ã¿åˆã‚ã›
    total_tests = len(TEST_QUESTIONS) * len(TEST_MODELS["structured_mermaid"]) * len(TEST_MODELS["answer_generation"])
    logger.info(f"ç·ãƒ†ã‚¹ãƒˆæ•°: {total_tests}")
    logger.info("")

    test_count = 0

    for q_info in TEST_QUESTIONS:
        for mermaid_model in TEST_MODELS["structured_mermaid"]:
            for answer_model in TEST_MODELS["answer_generation"]:
                test_count += 1
                logger.info(f"[{test_count}/{total_tests}] {q_info['id']}: {q_info['question']}")
                logger.info(f"  Mermaidãƒ¢ãƒ‡ãƒ«: {mermaid_model}, å›ç­”ãƒ¢ãƒ‡ãƒ«: {answer_model}")

                try:
                    # Step 1: ä¸­å¿ƒäººç‰©ç‰¹å®š
                    center_result = process_center_person(story_text, q_info['question'], character_summary)
                    center_person = center_result['content'].strip()

                    # Step 2: Structured Outputsã§é–¢ä¿‚å›³ç”Ÿæˆ
                    mermaid_result = process_structured_mermaid(
                        mermaid_model,
                        story_text,
                        q_info['question'],
                        center_person
                    )

                    # Step 3: å›ç­”ç”Ÿæˆ
                    answer_result = process_answer_generation(
                        answer_model,
                        story_text,
                        q_info['question'],
                        mermaid_result['content']
                    )

                    # Mermaidãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
                    mmd_file = output_dir / f"{q_info['id']}_{mermaid_model}_{answer_model}.mmd"
                    mmd_file.write_text(mermaid_result['content'], encoding='utf-8')

                    # çµæœè¨˜éŒ²
                    result = {
                        "question_id": q_info['id'],
                        "question": q_info['question'],
                        "question_type": q_info['type'],
                        "mermaid_model": mermaid_model,
                        "answer_model": answer_model,
                        "outputs": {
                            "center_person": center_person,
                            "mermaid_code": mermaid_result['content'],
                            "answer": answer_result['content'],
                            "relationships_count": len(mermaid_result['graph_data'].relationships)
                        },
                        "processes": {
                            "center_person": {
                                "time": center_result['time'],
                                "tokens": center_result['tokens']
                            },
                            "mermaid_generation": {
                                "time": mermaid_result['time'],
                                "tokens": mermaid_result['tokens']
                            },
                            "answer_generation": {
                                "time": answer_result['time'],
                                "tokens": answer_result['tokens']
                            }
                        },
                        "total_time": center_result['time'] + mermaid_result['time'] + answer_result['time']
                    }

                    results.append(result)
                    logger.info(f"  âœ… å®Œäº†: {result['total_time']:.2f}ç§’")

                except Exception as e:
                    logger.error(f"  âŒ ã‚¨ãƒ©ãƒ¼: {e}")
                    import traceback
                    traceback.print_exc()

                logger.info("")

    # çµæœä¿å­˜
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_json = f"model_comparison_structured_{timestamp}.json"
    with open(output_json, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)

    logger.info("=" * 80)
    logger.info(f"âœ¨ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Œäº†ï¼ çµæœ: {output_json}")
    logger.info(f"   ç·ãƒ†ã‚¹ãƒˆæ•°: {len(results)}/{total_tests}")
    logger.info("=" * 80)

if __name__ == "__main__":
    run_benchmark()
