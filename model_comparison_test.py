#!/usr/bin/env python3
# ===============================================
#  ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒå®Ÿé¨“ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
# ===============================================
import os
import json
import time
import csv
import re
import logging
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any
from dotenv import load_dotenv
import openai

# ç’°å¢ƒå¤‰æ•°èª­ã¿è¾¼ã¿
load_dotenv()
client = openai.OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.FileHandler(f'model_comparison_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# =================================================
#           ãƒ†ã‚¹ãƒˆè¨­å®š
# =================================================

# å›ºå®šãƒ¢ãƒ‡ãƒ«ï¼ˆå¿…ãšGPT-5.1ã‚’ä½¿ç”¨ï¼‰
FIXED_MODELS = {
    "character_judgment": "gpt-5.1",  # ç™»å ´äººç‰©åˆ¤å®š
    "center_person": "gpt-5.1"         # ä¸­å¿ƒäººç‰©ç‰¹å®š
}

# ãƒ†ã‚¹ãƒˆå¯¾è±¡ãƒ¢ãƒ‡ãƒ«
TEST_MODELS = {
    "mermaid_csv": [
        "gpt-5.1",
        "gpt-5-mini",
        "gpt-4-2025-08-07",
        "gpt-4.1",
        "gpt-4o",
        "gpt-4o-mini"
    ],
    "answer_generation": [
        "gpt-5-mini",
        "gpt-4-2025-08-07",
        "gpt-4.1",
        "gpt-4o",
        "gpt-4o-mini"
    ]
}

# ãƒ†ã‚¹ãƒˆç”¨ã®è³ªå•
TEST_QUESTIONS = [
    {
        "id": "Q1",
        "question": "ãƒŸãƒŠã£ã¦èª°ã ã£ã‘ï¼Ÿ",
        "type": "character_identification"
    },
    {
        "id": "Q2",
        "question": "ã‚¿ãƒ‹ã‚¢ã¨ã‚«ãƒŠãƒ‡ã®é–¢ä¿‚æ€§ã«ã¤ã„ã¦æ•™ãˆã¦",
        "type": "relationship"
    },
    {
        "id": "Q3",
        "question": "ãƒ¬ã‚¤ãƒ³ã¯ã‚¢ãƒªã‚ªã‚¹ã®ã“ã¨ãŒãªã‚“ã§å«Œã„ãªã®ï¼Ÿ",
        "type": "character_motivation"
    },
    {
        "id": "Q4",
        "question": "ã‚¿ãƒ‹ã‚¢ã¨ãƒªãƒ¼ãƒ³ã®é–¢ä¿‚æ€§ã«ã¤ã„ã¦æ•™ãˆã¦",
        "type": "relationship"
    }
]

# =================================================
#           ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°
# =================================================

def openai_chat_timed(model: str, messages: List[Dict], log_label: str = None, **kwargs) -> Dict[str, Any]:
    """
    OpenAI APIã‚’å‘¼ã³å‡ºã—ã€æ™‚é–“ã¨ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’è¨ˆæ¸¬

    Returns:
        dict: {
            "response": ChatCompletion,
            "time": float,
            "tokens": {"prompt": int, "completion": int, "total": int},
            "content": str
        }
    """
    start_time = time.time()

    try:
        response = client.chat.completions.create(
            model=model,
            messages=messages,
            **kwargs
        )
        elapsed = time.time() - start_time

        # ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡
        usage = response.usage
        tokens = {
            "prompt": usage.prompt_tokens if usage else 0,
            "completion": usage.completion_tokens if usage else 0,
            "total": usage.total_tokens if usage else 0,
            "cached": 0
        }

        # Prompt Cachingæƒ…å ±ã‚’å–å¾—
        if hasattr(usage, 'prompt_tokens_details') and hasattr(usage.prompt_tokens_details, 'cached_tokens'):
            tokens['cached'] = usage.prompt_tokens_details.cached_tokens

        content = response.choices[0].message.content if response.choices else ""

        log_msg = f"âœ“ {log_label or 'APIå‘¼ã³å‡ºã—'}: model={model}, time={elapsed:.2f}s, tokens={tokens['prompt']}â†’{tokens['completion']}"
        if tokens['cached'] > 0:
            log_msg += f" (cached: {tokens['cached']})"
        logger.info(log_msg)

        return {
            "response": response,
            "time": elapsed,
            "tokens": tokens,
            "content": content
        }

    except Exception as e:
        elapsed = time.time() - start_time
        logger.error(f"âœ— {log_label or 'APIå‘¼ã³å‡ºã—'}å¤±æ•—: model={model}, time={elapsed:.2f}s, error={str(e)}")
        return {
            "response": None,
            "time": elapsed,
            "tokens": {"prompt": 0, "completion": 0, "total": 0},
            "content": f"ERROR: {str(e)}",
            "error": str(e)
        }


def load_story_text() -> str:
    """
    å°èª¬æœ¬æ–‡ã‚’èª­ã¿è¾¼ã¿ï¼ˆ31ãƒšãƒ¼ã‚¸ã¾ã§ï¼‰

    zikken_11month_v7.pyã¨åŒã˜å½¢å¼ã§ã€31ãƒšãƒ¼ã‚¸ç›®ï¼ˆSTART_PAGE=30, å®Ÿéš›ã®ãƒšãƒ¼ã‚¸index=30ï¼‰ã¾ã§èª­ã¿è¾¼ã‚€
    """
    START_PAGE = 30  # zikken_11month_v7.pyã¨åŒã˜è¨­å®š
    story_file = Path(__file__).parent / "beast_text.json"

    if story_file.exists():
        with open(story_file, 'r', encoding='utf-8') as f:
            data = json.load(f)

        # zikken_11month_v7.pyã¨åŒã˜å½¢å¼ã§ãƒšãƒ¼ã‚¸ã‚’ä½œæˆ
        pages_all = [f"ã€{sec['section']}ç« ã€‘ {sec['title']}\n\n{sec['text']}"
                     for sec in data]

        # 31ãƒšãƒ¼ã‚¸ç›®ã¾ã§ï¼ˆindex 0-30 = 31ãƒšãƒ¼ã‚¸ï¼‰
        story_text = "\n\n".join(pages_all[:START_PAGE + 1])

        logger.info(f"å°èª¬æœ¬æ–‡ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {START_PAGE + 1}ãƒšãƒ¼ã‚¸, {len(story_text)}æ–‡å­—")
        return story_text
    else:
        logger.warning(f"å°èª¬ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {story_file}")
        return "ï¼ˆã‚µãƒ³ãƒ—ãƒ«ãƒ†ã‚­ã‚¹ãƒˆï¼‰ã“ã‚Œã¯å°èª¬ã®æœ¬æ–‡ã§ã™ã€‚"


def load_character_summary() -> str:
    """ç™»å ´äººç‰©è¦ç´„ã‚’èª­ã¿è¾¼ã¿"""
    summary_file = Path(__file__).parent / "character_summary.txt"
    if summary_file.exists():
        with open(summary_file, 'r', encoding='utf-8') as f:
            return f.read()
    else:
        logger.warning(f"ç™»å ´äººç‰©è¦ç´„ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {summary_file}")
        return ""


# =================================================
#           å„ãƒ—ãƒ­ã‚»ã‚¹ã®å®Ÿè¡Œé–¢æ•°
# =================================================

def process_character_judgment(story_text: str, question: str, character_summary: str) -> Dict[str, Any]:
    """
    ç™»å ´äººç‰©åˆ¤å®šãƒ—ãƒ­ã‚»ã‚¹ï¼ˆå›ºå®š: GPT-5.1ï¼‰
    """
    prompt = f"""ä»¥ä¸‹ã®æ–‡ç« ã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒä»Šã¾ã§èª­ã‚“ã å°èª¬æœ¬æ–‡ã§ã™ã€‚
----- æœ¬æ–‡ã“ã“ã‹ã‚‰ -----
{story_text}
----- æœ¬æ–‡ã“ã“ã¾ã§ -----

ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ä»¥ä¸‹ã®è³ªå•ãŒã‚ã‚Šã¾ã—ãŸã€‚
è³ªå•: {question}

ã“ã®è³ªå•ãŒã€Œç™»å ´äººç‰©ã«é–¢ã™ã‚‹è³ªå•ã€ã‹ã©ã†ã‹ã‚’åˆ¤å®šã—ã¦ãã ã•ã„ã€‚
- ç™»å ´äººç‰©ã®é–¢ä¿‚ã€è¡Œå‹•ã€æ€§æ ¼ã€èƒŒæ™¯ãªã©ã«é–¢ã™ã‚‹è³ªå•ãªã‚‰ã€Œã¯ã„ã€
- ãã‚Œä»¥å¤–ãªã‚‰ã€Œã„ã„ãˆã€

JSONå½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„:
{{"is_character_question": "ã¯ã„" or "ã„ã„ãˆ", "reason": "ç†ç”±"}}"""

    messages = [
        {"role": "system", "content": "ã‚ãªãŸã¯è³ªå•ã‚’åˆ†é¡ã™ã‚‹ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"},
        {"role": "user", "content": prompt}
    ]

    return openai_chat_timed(
        model=FIXED_MODELS["character_judgment"],
        messages=messages,
        log_label="ç™»å ´äººç‰©åˆ¤å®š",
        temperature=0.0
    )


def process_center_person(story_text: str, question: str, character_summary: str) -> Dict[str, Any]:
    """
    ä¸­å¿ƒäººç‰©ç‰¹å®šãƒ—ãƒ­ã‚»ã‚¹ï¼ˆå›ºå®š: GPT-5.1ï¼‰
    zikken_11month_v7.pyã¨åŒã˜ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
    """
    prompt = f"""ç™»å ´äººç‰©æƒ…å ±:
{character_summary}

---

è³ªå•: {question}

ã“ã®è³ªå•ã®ä¸­å¿ƒã¨ãªã‚‹ç™»å ´äººç‰©ã®åå‰ã‚’1ã¤ã ã‘ç­”ãˆã¦ãã ã•ã„ã€‚

è¦ä»¶:
- ç™»å ´äººç‰©æƒ…å ±ã«è¨˜è¼‰ã•ã‚Œã¦ã„ã‚‹æ­£ç¢ºãªäººç‰©åã§å›ç­”
- äººç‰©åã®ã¿ã‚’1è¡Œã§å‡ºåŠ›ï¼ˆèª¬æ˜ä¸è¦ï¼‰

å›ç­”:"""

    messages = [
        {"role": "system", "content": "è³ªå•ã®ä¸­å¿ƒäººç‰©ã‚’ç‰¹å®šã—ã¾ã™ã€‚"},
        {"role": "user", "content": prompt}
    ]

    return openai_chat_timed(
        model=FIXED_MODELS["center_person"],
        messages=messages,
        log_label="ä¸­å¿ƒäººç‰©ç‰¹å®š",
        temperature=0.0
    )


def process_mermaid_generation(model: str, story_text: str, question: str,
                                center_person: str, character_summary: str) -> Dict[str, Any]:
    """
    Mermaidå›³ç”Ÿæˆãƒ—ãƒ­ã‚»ã‚¹ï¼ˆãƒ©ãƒ•ç”Ÿæˆï¼‰
    zikken_11month_v7.pyã¨åŒã˜ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
    """
    prompt = f"""ã‚¿ã‚¹ã‚¯: ä»¥ä¸‹ã®æœ¬æ–‡ã‚’èª­ã¿ã€è³ªå•ã§æŒ‡å®šã•ã‚ŒãŸäººç‰©ã‚’ä¸­å¿ƒã¨ã—ãŸç™»å ´äººç‰©ã®é–¢ä¿‚å›³ã‚’Mermaidå½¢å¼ã§ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

ã€é‡è¦ãªæ³¨æ„äº‹é …ã€‘
âŒ é–“é•ã£ãŸä¾‹ï¼ˆçµ¶å¯¾ã«ã‚„ã£ã¦ã¯ã„ã‘ãªã„ï¼‰:
```mermaid
graph LR
    ä¸æ˜((ä¸æ˜))
    ä¸æ˜ --- ãƒŸãƒŠ
    ä¸æ˜ -->|è³ªå•å¯¾è±¡| ãƒŸãƒŠ
```
ã“ã®ã‚ˆã†ã«ã€Œä¸æ˜ã€ã€Œè³ªå•è€…ã€ãªã©ã®æŠ½è±¡çš„ãªãƒãƒ¼ãƒ‰ã‚’ä½œæˆã—ã¦ã¯ã„ã‘ã¾ã›ã‚“ã€‚

âœ… æ­£ã—ã„ä¾‹:
```mermaid
graph LR
    ãƒŸãƒŠ[ãƒŸãƒŠ<br/>ç¥å®˜]
    ã‚¢ãƒªã‚ªã‚¹[ã‚¢ãƒªã‚ªã‚¹<br/>å‹‡è€…]
    ãƒ¬ã‚¤ãƒ³[ãƒ¬ã‚¤ãƒ³<br/>ãƒ“ãƒ¼ã‚¹ãƒˆãƒ†ã‚¤ãƒãƒ¼]

    ãƒŸãƒŠ <--> ã‚¢ãƒªã‚ªã‚¹[ä»²é–“]
    ãƒŸãƒŠ <--> ãƒ¬ã‚¤ãƒ³[å…ƒä»²é–“]
```
ã“ã®ã‚ˆã†ã«å®Ÿåœ¨ã™ã‚‹ç™»å ´äººç‰©ã®ã¿ã‚’ãƒãƒ¼ãƒ‰ã¨ã—ã¦ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚

æœ¬æ–‡:
{story_text}

è³ªå•: {question}

è¦ä»¶:
- graph LR ã¾ãŸã¯ graph TD ã§é–‹å§‹
- **{center_person}ã‚’ä¸­å¿ƒ**ã«ã€ç›´æ¥é–¢ã‚ã‚‹ä¸»è¦äººç‰©ã®ã¿ã‚’å«ã‚ã‚‹
- ç™»å ´äººç‰©ã¯ç‰©èªä¸Šé‡è¦ãªäººç‰©ã«é™å®šã™ã‚‹ï¼ˆ5-10äººç¨‹åº¦ï¼‰
- é–¢ä¿‚æ€§ã®è¡¨ç¾ï¼š
  * åŒæ–¹å‘ã®é–¢ä¿‚: <--> ã‚’ä½¿ç”¨ï¼ˆä¾‹: å‹äººã€ä»²é–“ã€æ‹äººãªã©ï¼‰
  * ä¸€æ–¹å‘ã®é–¢ä¿‚: --> ã‚’ä½¿ç”¨ï¼ˆä¾‹: ä¸Šå¸â†’éƒ¨ä¸‹ã€å¸«åŒ â†’å¼Ÿå­ãªã©ï¼‰
  * ç‚¹ç·šçŸ¢å° -.-> ã¯è£œåŠ©çš„ãªé–¢ä¿‚ã«ä½¿ç”¨
- **é‡è¦**: åŒã˜2äººã®é–“ã®é–¢ä¿‚ã¯æœ€å¤§2æœ¬ã¾ã§ï¼ˆAã‹ã‚‰Bã€Bã‹ã‚‰Aï¼‰
- ã‚¨ãƒƒã‚¸ã«ã¯ç°¡æ½”ãªæ—¥æœ¬èªãƒ©ãƒ™ãƒ«ã‚’ä»˜ã‘ã‚‹ï¼ˆ5æ–‡å­—ä»¥å†…æ¨å¥¨ï¼‰
- å¿…è¦ã«å¿œã˜ã¦subgraphã§ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ï¼ˆä¾‹: å‹‡è€…ãƒ‘ãƒ¼ãƒ†ã‚£ãƒ¼ã€é­”ç‹è»ãªã©ï¼‰
- {center_person}ã«ç›´æ¥é–¢ã‚ã‚‰ãªã„äººç‰©é–“ã®é–¢ä¿‚ã¯çœç•¥ã™ã‚‹

**çµ¶å¯¾ã«å®ˆã‚‹ã“ã¨:**
1. ã€Œä¸æ˜ã€ã€Œè³ªå•è€…ã€ã€Œä¸»ä½“ã€ã€Œå®¢ä½“ã€ãªã©ã®æŠ½è±¡çš„ãªãƒãƒ¼ãƒ‰ã¯çµ¶å¯¾ã«ä½œæˆã—ãªã„
2. å¿…ãšå®Ÿåœ¨ã™ã‚‹ç™»å ´äººç‰©ã®ã¿ã‚’ãƒãƒ¼ãƒ‰ã¨ã—ã¦ä½¿ç”¨ã™ã‚‹
3. {center_person}è‡ªèº«ã‚’ãƒãƒ¼ãƒ‰ã¨ã—ã¦å¿…ãšå«ã‚ã‚‹
4. è³ªå•ã®æ„å›³ã¯ã€Œ{center_person}ã«ã¤ã„ã¦ã®é–¢ä¿‚å›³ã‚’ä½œã‚‹ã€ã§ã‚ã£ã¦ã€Œä¸æ˜ãªäººç‰©ãŒ{center_person}ã«ã¤ã„ã¦è³ªå•ã™ã‚‹å›³ã€ã§ã¯ãªã„

å‡ºåŠ›ã¯Mermaidã‚³ãƒ¼ãƒ‰ã®ã¿ï¼ˆèª¬æ˜ä¸è¦ï¼‰"""

    messages = [
        {"role": "system", "content": "Mermaidå›³ã‚’ç”Ÿæˆã™ã‚‹å°‚é–€å®¶ã§ã™ã€‚"},
        {"role": "user", "content": prompt}
    ]

    # gpt-5ç³»ã¯temperatureã‚’ã‚µãƒãƒ¼ãƒˆã—ãªã„ãŸã‚ã€ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã£ã¦åˆ†å²
    kwargs = {"log_label": f"Mermaidç”Ÿæˆ({model})"}
    if "gpt-5" not in model:
        kwargs["temperature"] = 0.3

    return openai_chat_timed(
        model=model,
        messages=messages,
        **kwargs
    )


def process_csv_conversion(model: str, mermaid_code: str, story_text: str, center_person: str) -> Dict[str, Any]:
    """
    CSVå¤‰æ›ãƒ—ãƒ­ã‚»ã‚¹
    zikken_11month_v7.pyã¨åŒã˜ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
    """
    prompt = f"""æœ¬æ–‡ï¼ˆå‚è€ƒï¼‰:
{story_text}

Mermaidå›³:
{mermaid_code}

å‡ºåŠ›å½¢å¼:
ä¸»ä½“,é–¢ä¿‚ã‚¿ã‚¤ãƒ—,é–¢ä¿‚è©³ç´°,å®¢ä½“,ã‚°ãƒ«ãƒ¼ãƒ—

èª¬æ˜:
- ä¸»ä½“: é–¢ä¿‚ã®èµ·ç‚¹ã¨ãªã‚‹äººç‰©
- é–¢ä¿‚ã‚¿ã‚¤ãƒ—: directedï¼ˆä¸€æ–¹å‘ï¼‰, bidirectionalï¼ˆåŒæ–¹å‘ï¼‰, dottedï¼ˆç‚¹ç·šï¼‰
- é–¢ä¿‚è©³ç´°: é–¢ä¿‚ã‚’è¡¨ã™æ—¥æœ¬èªï¼ˆ5æ–‡å­—ä»¥å†…ï¼‰
- å®¢ä½“: é–¢ä¿‚ã®çµ‚ç‚¹ã¨ãªã‚‹äººç‰©
- ã‚°ãƒ«ãƒ¼ãƒ—: subgraphã«å±ã™ã‚‹å ´åˆã¯ã‚°ãƒ«ãƒ¼ãƒ—åã€ãªã‘ã‚Œã°ç©ºæ¬„

é‡è¦ãªåˆ¶ç´„:
- **åŒã˜2äººã®é–“ã®é–¢ä¿‚ã¯æœ€å¤§2æœ¬ã¾ã§**ï¼ˆAâ†’B ã¨ Bâ†’A ã®ã¿ï¼‰
- åŒã˜æ–¹å‘ã®é‡è¤‡ã™ã‚‹é–¢ä¿‚ã¯1ã¤ã«ã¾ã¨ã‚ã‚‹
- æœ¬æ–‡ã«å­˜åœ¨ã—ãªã„äººç‰©é–¢ä¿‚ã¯é™¤å¤–
- {center_person}ã«ç›´æ¥é–¢ã‚ã‚‹äººç‰©ã‚’å„ªå…ˆ
- {center_person}ã«ç›´æ¥é–¢ã‚ã‚‰ãªã„äººç‰©é–“ã®é–¢ä¿‚ã¯çœç•¥
- ãƒ˜ãƒƒãƒ€ãƒ¼ã¯ä¸è¦

ä»¥ä¸Šã®Mermaidå›³ã‹ã‚‰ã€Œ{center_person}ã€ã‚’ä¸­å¿ƒã¨ã—ãŸä¸»è¦äººç‰©ã®é–¢ä¿‚ã®ã¿ã‚’æŠ½å‡ºã—ã¦CSVå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚"""

    messages = [
        {"role": "system", "content": "Mermaidå›³ã¨æœ¬æ–‡ã‚’ç…§åˆã—ã¦æ­£ç¢ºãªé–¢ä¿‚ã‚’æŠ½å‡ºã—ã¾ã™ã€‚"},
        {"role": "user", "content": prompt}
    ]

    # gpt-5ç³»ã¯temperatureã‚’ã‚µãƒãƒ¼ãƒˆã—ãªã„ãŸã‚ã€ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã£ã¦åˆ†å²
    kwargs = {"log_label": f"CSVå¤‰æ›({model})"}
    if "gpt-5" not in model:
        kwargs["temperature"] = 0.0

    return openai_chat_timed(
        model=model,
        messages=messages,
        **kwargs
    )


def build_mermaid_from_csv(csv_text: str, main_focus: str = None) -> str:
    """
    CSVãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æ­£ç¢ºãªMermaidå›³ã‚’æ§‹ç¯‰
    é‡è¤‡ã™ã‚‹é–¢ä¿‚ã‚’çµ±åˆã—ã€åŒã˜ãƒšã‚¢é–“ã®é–¢ä¿‚ã‚’æœ€å¤§2æœ¬ï¼ˆåŒæ–¹å‘ï¼‰ã«åˆ¶é™

    æ”¹å–„ç‚¹:
    - CSVãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—
    - ãƒ¡ã‚¿ãƒãƒ¼ãƒ‰ï¼ˆä¸æ˜ã€ä¸»ä½“ã€å®¢ä½“ãªã©ï¼‰ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    - ä¸­å¿ƒäººç‰©ã®åå‰ãƒãƒƒãƒãƒ³ã‚°ã‚’æ”¹å–„ï¼ˆéƒ¨åˆ†ä¸€è‡´ï¼‰
    """
    # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¯¾è±¡ã®ãƒ¡ã‚¿ãƒãƒ¼ãƒ‰
    INVALID_NODES = {
        'ä¸æ˜', 'ä¸»ä½“', 'å®¢ä½“', 'ã‚°ãƒ«ãƒ¼ãƒ—', 'é–¢ä¿‚ã‚¿ã‚¤ãƒ—', 'é–¢ä¿‚è©³ç´°',
        '?', 'ï¼Ÿ', 'None', 'none', 'null', 'NULL', ''
    }

    # ãƒãƒ¼ãƒ‰ã¨ã‚¨ãƒƒã‚¸ã®åé›†
    nodes = set()
    edges = []
    groups = {}  # ã‚°ãƒ«ãƒ¼ãƒ—å -> ãƒãƒ¼ãƒ‰ãƒªã‚¹ãƒˆ
    edge_map = {}  # (src, dst)ã®ãƒšã‚¢ã‚’ã‚­ãƒ¼ã«ã—ã¦é‡è¤‡ãƒã‚§ãƒƒã‚¯

    reader = csv.reader(csv_text.splitlines())
    for i, row in enumerate(reader):
        # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—
        if i == 0 and row[0].strip() in ['ä¸»ä½“', 'é–¢ä¿‚', 'source', 'Source']:
            continue

        if len(row) < 4:
            continue

        src = row[0].strip()
        rel_type = row[1].strip() if len(row) > 1 else "directed"
        rel_label = row[2].strip() if len(row) > 2 else "é–¢ä¿‚"
        dst = row[3].strip() if len(row) > 3 else ""
        group = row[4].strip() if len(row) > 4 else ""

        # ãƒ¡ã‚¿ãƒãƒ¼ãƒ‰ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        if src in INVALID_NODES or dst in INVALID_NODES:
            continue

        if not src or not dst:
            continue

        # åŒã˜ãƒšã‚¢ï¼ˆé †åºã‚ã‚Šï¼‰ã®é‡è¤‡ãƒã‚§ãƒƒã‚¯
        edge_key = (src, dst)
        if edge_key in edge_map:
            # æ—¢ã«åŒã˜æ–¹å‘ã®é–¢ä¿‚ãŒã‚ã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
            continue

        nodes.add(src)
        nodes.add(dst)

        # ã‚°ãƒ«ãƒ¼ãƒ—ã®è¨˜éŒ²
        if group:
            if group not in groups:
                groups[group] = set()
            groups[group].add(src)
            groups[group].add(dst)

        # ã‚¨ãƒƒã‚¸ã®è¨˜éŒ²
        edge_symbol = "-->"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
        if rel_type.lower() in ["bidirectional", "åŒæ–¹å‘"]:
            edge_symbol = "<-->"
        elif rel_type.lower() in ["dotted", "ç‚¹ç·š"]:
            edge_symbol = "-.->"

        edges.append({
            "src": src,
            "dst": dst,
            "symbol": edge_symbol,
            "label": rel_label[:5]  # 5æ–‡å­—åˆ¶é™
        })
        edge_map[edge_key] = True

    # Mermaidå›³ã®æ§‹ç¯‰
    lines = ["graph LR"]

    # ãƒãƒ¼ãƒ‰IDã®ç”Ÿæˆï¼ˆå®‰å…¨ãªè­˜åˆ¥å­ï¼‰
    def safe_id(name: str) -> str:
        return f'id_{abs(hash(name)) % 10000}'

    node_ids = {name: safe_id(name) for name in nodes}

    # ãƒãƒ¼ãƒ‰å®šç¾©
    for name in sorted(nodes):
        node_id = node_ids[name]
        lines.append(f'    {node_id}["{name}"]')

    # ã‚µãƒ–ã‚°ãƒ©ãƒ•ã®å®šç¾©
    if groups:
        for group_name, group_nodes in groups.items():
            safe_group_name = re.sub(r'[^0-9A-Za-z_\u3040-\u30FF\u4E00-\u9FFF\s]', '', group_name)
            lines.append(f'\n    subgraph {safe_group_name}')
            for node in group_nodes:
                if node in node_ids:
                    lines.append(f'        {node_ids[node]}')
            lines.append('    end')

    # ã‚¨ãƒƒã‚¸ã®å®šç¾©
    lines.append('')  # ç©ºè¡Œ
    for edge in edges:
        if edge["src"] in node_ids and edge["dst"] in node_ids:
            src_id = node_ids[edge["src"]]
            dst_id = node_ids[edge["dst"]]

            if edge["label"]:
                if edge["symbol"] == "<-->":
                    lines.append(f'    {src_id} <-->|{edge["label"]}| {dst_id}')
                elif edge["symbol"] == "-.->":
                    lines.append(f'    {src_id} -.->|{edge["label"]}| {dst_id}')
                else:
                    lines.append(f'    {src_id} -->|{edge["label"]}| {dst_id}')
            else:
                lines.append(f'    {src_id} {edge["symbol"]} {dst_id}')

    # ä¸­å¿ƒäººç‰©ã®å¼·èª¿ï¼ˆéƒ¨åˆ†ä¸€è‡´ã§æ¤œç´¢ï¼‰
    if main_focus:
        # main_focusãŒå®Œå…¨ä¸€è‡´ã§å­˜åœ¨ã™ã‚‹ã‹ç¢ºèª
        if main_focus in node_ids:
            lines.append(f'\n    style {node_ids[main_focus]} fill:#FFD700,stroke:#FF8C00,stroke-width:4px')
        else:
            # éƒ¨åˆ†ä¸€è‡´ã§æ¤œç´¢ï¼ˆä¾‹: "ãƒ¬ã‚¤ãƒ³ãƒ»ã‚·ãƒ¥ãƒ©ã‚¦ãƒ‰" ãŒ "ãƒ¬ã‚¤ãƒ³" ã«ãƒãƒƒãƒï¼‰
            for node_name in node_ids:
                if main_focus in node_name or node_name in main_focus:
                    lines.append(f'\n    style {node_ids[node_name]} fill:#FFD700,stroke:#FF8C00,stroke-width:4px')
                    break  # æœ€åˆã«ãƒãƒƒãƒã—ãŸãƒãƒ¼ãƒ‰ã®ã¿ã‚’ãƒã‚¤ãƒ©ã‚¤ãƒˆ

    return '\n'.join(lines)


def process_answer_generation(model: str, story_text: str, question: str) -> Dict[str, Any]:
    """
    è³ªå•ã¸ã®å›ç­”ç”Ÿæˆãƒ—ãƒ­ã‚»ã‚¹
    """
    prompt = f"""ä»¥ä¸‹ã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒã“ã‚Œã¾ã§ã«èª­ã‚“ã å°èª¬æœ¬æ–‡ã§ã™ã€‚

----- æœ¬æ–‡ã“ã“ã‹ã‚‰ -----
{story_text}
----- æœ¬æ–‡ã“ã“ã¾ã§ -----

# æŒ‡ç¤º
ã“ã®æœ¬æ–‡ã®å†…å®¹ã‚’æ ¹æ‹ ã«ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«æ—¥æœ¬èªã§ä¸å¯§ã«ç­”ãˆã¦ãã ã•ã„ã€‚

è³ªå•: {question}"""

    messages = [
        {"role": "system", "content": "ã‚ãªãŸã¯èª­ã‚“ã§ã„ã‚‹å°èª¬ã«ã¤ã„ã¦è³ªå•ã«ç­”ãˆã‚‹ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"},
        {"role": "user", "content": prompt}
    ]

    # gpt-5ç³»ã¯temperatureã‚’ã‚µãƒãƒ¼ãƒˆã—ãªã„ãŸã‚ã€ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã£ã¦åˆ†å²
    kwargs = {"log_label": f"å›ç­”ç”Ÿæˆ({model})"}
    if "gpt-5" not in model:
        kwargs["temperature"] = 0.7

    return openai_chat_timed(
        model=model,
        messages=messages,
        **kwargs
    )


# =================================================
#           ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
# =================================================

def run_single_test(question_data: Dict, mermaid_model: str, answer_model: str,
                    story_text: str, character_summary: str, test_num: int = 0, total_tests: int = 0) -> Dict[str, Any]:
    """
    1ã¤ã®ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã‚’å®Ÿè¡Œ
    """
    question_id = question_data["id"]
    question = question_data["question"]

    progress_info = f"[{test_num}/{total_tests}]" if total_tests > 0 else ""
    logger.info(f"\n{'='*60}")
    logger.info(f"{progress_info} ãƒ†ã‚¹ãƒˆé–‹å§‹: {question_id} - Mermaid/CSV={mermaid_model}, Answer={answer_model}")
    logger.info(f"è³ªå•: {question}")
    logger.info(f"{'='*60}")

    results = {
        "question_id": question_id,
        "question": question,
        "question_type": question_data["type"],
        "mermaid_model": mermaid_model,
        "answer_model": answer_model,
        "processes": {},
        "outputs": {},
        "total_time": 0,
        "timestamp": datetime.now().isoformat()
    }

    # 1. ç™»å ´äººç‰©åˆ¤å®šï¼ˆå›ºå®š: GPT-5.1ï¼‰
    judgment_result = process_character_judgment(story_text, question, character_summary)
    results["processes"]["character_judgment"] = {
        "model": FIXED_MODELS["character_judgment"],
        "time": judgment_result["time"],
        "tokens": judgment_result["tokens"]
    }
    results["outputs"]["character_judgment"] = judgment_result["content"]

    # 2. ä¸­å¿ƒäººç‰©ç‰¹å®šï¼ˆå›ºå®š: GPT-5.1ï¼‰
    center_result = process_center_person(story_text, question, character_summary)
    results["processes"]["center_person"] = {
        "model": FIXED_MODELS["center_person"],
        "time": center_result["time"],
        "tokens": center_result["tokens"]
    }
    results["outputs"]["center_person"] = center_result["content"]

    # ä¸­å¿ƒäººç‰©ã‚’æŠ½å‡ºï¼ˆç°¡æ˜“ç‰ˆï¼‰
    try:
        center_data = json.loads(center_result["content"])
        center_person = center_data.get("center_person", "ä¸æ˜")
    except:
        center_person = "ä¸æ˜"

    # 3. Mermaidç”Ÿæˆ
    mermaid_result = process_mermaid_generation(
        mermaid_model, story_text, question, center_person, character_summary
    )
    results["processes"]["mermaid_generation"] = {
        "model": mermaid_model,
        "time": mermaid_result["time"],
        "tokens": mermaid_result["tokens"]
    }
    results["outputs"]["mermaid_rough"] = mermaid_result["content"]  # ãƒ©ãƒ•Mermaidã‚’ä¿å­˜

    # 4. CSVå¤‰æ›
    csv_result = process_csv_conversion(mermaid_model, mermaid_result["content"], story_text, center_person)
    results["processes"]["csv_conversion"] = {
        "model": mermaid_model,
        "time": csv_result["time"],
        "tokens": csv_result["tokens"]
    }
    results["outputs"]["csv"] = csv_result["content"]

    # 5. CSVã‹ã‚‰Mermaidå†æ§‹ç¯‰ï¼ˆãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ï¼‰
    try:
        rebuilt_mermaid = build_mermaid_from_csv(csv_result["content"], center_person)
        results["outputs"]["mermaid_code"] = rebuilt_mermaid  # æœ€çµ‚Mermaidã‚’ä¿å­˜
        logger.debug(f"Mermaidå†æ§‹ç¯‰å®Œäº†: {len(rebuilt_mermaid)} æ–‡å­—")
    except Exception as e:
        logger.error(f"Mermaidå†æ§‹ç¯‰ã‚¨ãƒ©ãƒ¼: {e}")
        # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ãƒ©ãƒ•Mermaidã‚’ãã®ã¾ã¾ä½¿ç”¨
        results["outputs"]["mermaid_code"] = mermaid_result["content"]

    # Mermaidãƒ»CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ï¼ˆãƒ©ãƒ•ã€CSVã€æ•´å½¢å¾Œã®3ã¤ï¼‰
    try:
        mermaid_dir = Path(__file__).parent / "mermaid_outputs"
        mermaid_dir.mkdir(exist_ok=True)

        # ãƒ•ã‚¡ã‚¤ãƒ«åã®ç”Ÿæˆ
        base_filename = f"{question_id}_{mermaid_model}_{answer_model}"

        # ãƒ©ãƒ•Mermaid
        rough_file = mermaid_dir / f"{base_filename}_rough.mmd"
        with open(rough_file, 'w', encoding='utf-8') as f:
            f.write(results["outputs"]["mermaid_rough"])

        # CSV
        csv_file = mermaid_dir / f"{base_filename}.csv"
        with open(csv_file, 'w', encoding='utf-8') as f:
            f.write(results["outputs"]["csv"])

        # æ•´å½¢å¾ŒMermaid
        final_file = mermaid_dir / f"{base_filename}.mmd"
        with open(final_file, 'w', encoding='utf-8') as f:
            f.write(results["outputs"]["mermaid_code"])

        logger.debug(f"ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜: {base_filename} (rough.mmd, .csv, .mmd)")
    except Exception as e:
        logger.warning(f"ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

    # 6. å›ç­”ç”Ÿæˆ
    answer_result = process_answer_generation(answer_model, story_text, question)
    results["processes"]["answer_generation"] = {
        "model": answer_model,
        "time": answer_result["time"],
        "tokens": answer_result["tokens"]
    }
    results["outputs"]["answer"] = answer_result["content"]

    # åˆè¨ˆæ™‚é–“
    results["total_time"] = sum(p["time"] for p in results["processes"].values())

    # ãƒ—ãƒ­ã‚»ã‚¹åˆ¥ã®æ™‚é–“å†…è¨³ã‚’è¡¨ç¤º
    process_times = {
        "ç™»å ´äººç‰©åˆ¤å®š": results["processes"]["character_judgment"]["time"],
        "ä¸­å¿ƒäººç‰©ç‰¹å®š": results["processes"]["center_person"]["time"],
        "Mermaidç”Ÿæˆ": results["processes"]["mermaid_generation"]["time"],
        "CSVå¤‰æ›": results["processes"]["csv_conversion"]["time"],
        "å›ç­”ç”Ÿæˆ": results["processes"]["answer_generation"]["time"]
    }

    logger.info(f"\n{'='*60}")
    logger.info(f"{progress_info} ãƒ†ã‚¹ãƒˆå®Œäº†: {question_id}")
    logger.info(f"åˆè¨ˆæ™‚é–“: {results['total_time']:.2f}s")
    logger.info(f"å†…è¨³: " + " | ".join([f"{k}={v:.2f}s" for k, v in process_times.items()]))
    logger.info(f"{'='*60}\n")

    return results


def warmup_prompt_cache(story_text: str, character_summary: str):
    """
    ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ï¼ˆzikken_11month_v7.pyã¨åŒã˜å‡¦ç†ï¼‰

    ã“ã‚Œã«ã‚ˆã‚Šã€æœ€åˆã®ãƒ†ã‚¹ãƒˆã‹ã‚‰é«˜é€Ÿãªå¿œç­”ãŒå¯èƒ½ã«ãªã‚‹
    """
    logger.info("\n" + "=" * 80)
    logger.info("ğŸ”¥ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ä¸­...")
    logger.info("=" * 80)

    try:
        # 1. æœ¬æ–‡ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä½œæˆ
        warmup_prompt_story = f"""
æœ¬æ–‡:
{story_text}

è³ªå•: ä¸»äººå…¬ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„

è¦ä»¶:
- graph LR ã¾ãŸã¯ graph TD ã§é–‹å§‹
- **ä¸»äººå…¬ã‚’ä¸­å¿ƒ**ã«ã€ç›´æ¥é–¢ã‚ã‚‹ä¸»è¦äººç‰©ã®ã¿ã‚’å«ã‚ã‚‹
- ç™»å ´äººç‰©ã¯ç‰©èªä¸Šé‡è¦ãªäººç‰©ã«é™å®šã™ã‚‹ï¼ˆ5-10äººç¨‹åº¦ï¼‰
- é–¢ä¿‚æ€§ã®è¡¨ç¾ï¼š
  * åŒæ–¹å‘ã®é–¢ä¿‚: <--> ã‚’ä½¿ç”¨ï¼ˆä¾‹: å‹äººã€ä»²é–“ã€æ‹äººãªã©ï¼‰
  * ä¸€æ–¹å‘ã®é–¢ä¿‚: --> ã‚’ä½¿ç”¨ï¼ˆä¾‹: ä¸Šå¸â†’éƒ¨ä¸‹ã€å¸«åŒ â†’å¼Ÿå­ãªã©ï¼‰
  * ç‚¹ç·šçŸ¢å° -.-> ã¯è£œåŠ©çš„ãªé–¢ä¿‚ã«ä½¿ç”¨
- **é‡è¦**: åŒã˜2äººã®é–“ã®é–¢ä¿‚ã¯æœ€å¤§2æœ¬ã¾ã§ï¼ˆAã‹ã‚‰Bã€Bã‹ã‚‰Aï¼‰
- ã‚¨ãƒƒã‚¸ã«ã¯ç°¡æ½”ãªæ—¥æœ¬èªãƒ©ãƒ™ãƒ«ã‚’ä»˜ã‘ã‚‹ï¼ˆ5æ–‡å­—ä»¥å†…æ¨å¥¨ï¼‰
- å¿…è¦ã«å¿œã˜ã¦subgraphã§ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ï¼ˆä¾‹: å‹‡è€…ãƒ‘ãƒ¼ãƒ†ã‚£ãƒ¼ã€é­”ç‹è»ãªã©ï¼‰
- ä¸»äººå…¬ã«ç›´æ¥é–¢ã‚ã‚‰ãªã„äººç‰©é–“ã®é–¢ä¿‚ã¯çœç•¥ã™ã‚‹

ä»¥ä¸Šã®è³ªå•ã¨æœ¬æ–‡ã‚’åŸºã«ã€ã€Œä¸»äººå…¬ã€ã‚’ä¸­å¿ƒã¨ã—ãŸä¸»è¦ç™»å ´äººç‰©ã®é–¢ä¿‚å›³ã‚’Mermaidå½¢å¼ã§ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
å‡ºåŠ›ã¯Mermaidã‚³ãƒ¼ãƒ‰ã®ã¿ï¼ˆèª¬æ˜ä¸è¦ï¼‰
"""

        logger.info("æœ¬æ–‡ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä½œæˆä¸­...")
        _ = openai_chat_timed(
            "gpt-5.1",
            messages=[
                {"role": "system", "content": "Mermaidå›³ã‚’ç”Ÿæˆã™ã‚‹å°‚é–€å®¶ã§ã™ã€‚"},
                {"role": "user", "content": warmup_prompt_story}
            ],
            temperature=0.3,
            log_label="ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ï¼ˆæœ¬æ–‡ï¼‰"
        )

        # 2. ç™»å ´äººç‰©æƒ…å ±ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä½œæˆ
        if character_summary:
            warmup_prompt_character = f"""
ç™»å ´äººç‰©æƒ…å ±:
{character_summary}

---

è³ªå•: ä¸»äººå…¬ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„

ã“ã®è³ªå•ã®ä¸­å¿ƒã¨ãªã‚‹ç™»å ´äººç‰©ã®åå‰ã‚’1ã¤ã ã‘ç­”ãˆã¦ãã ã•ã„ã€‚

è¦ä»¶:
- ç™»å ´äººç‰©æƒ…å ±ã«è¨˜è¼‰ã•ã‚Œã¦ã„ã‚‹æ­£ç¢ºãªäººç‰©åã§å›ç­”
- äººç‰©åã®ã¿ã‚’1è¡Œã§å‡ºåŠ›ï¼ˆèª¬æ˜ä¸è¦ï¼‰

å›ç­”:
"""

            logger.info("ç™»å ´äººç‰©æƒ…å ±ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä½œæˆä¸­...")
            _ = openai_chat_timed(
                "gpt-5.1",
                messages=[
                    {"role": "system", "content": "è³ªå•ã®ä¸­å¿ƒäººç‰©ã‚’ç‰¹å®šã—ã¾ã™ã€‚"},
                    {"role": "user", "content": warmup_prompt_character}
                ],
                temperature=0,
                log_label="ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ï¼ˆç™»å ´äººç‰©ï¼‰"
            )

        logger.info("âœ… ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ãŒå®Œäº†ã—ã¾ã—ãŸ")
        logger.info("=" * 80 + "\n")

    except Exception as e:
        logger.error(f"âŒ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        logger.error("ãƒ†ã‚¹ãƒˆã‚’ä¸­æ­¢ã—ã¾ã™")
        raise


def run_all_tests():
    """
    å…¨ã¦ã®ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã‚’å®Ÿè¡Œ
    """
    logger.info("=" * 80)
    logger.info("ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒå®Ÿé¨“ã‚’é–‹å§‹ã—ã¾ã™")
    logger.info("=" * 80)

    # å°èª¬æœ¬æ–‡ã¨ç™»å ´äººç‰©è¦ç´„ã‚’èª­ã¿è¾¼ã¿
    story_text = load_story_text()
    character_summary = load_character_summary()

    logger.info(f"å°èª¬æœ¬æ–‡: {len(story_text)} æ–‡å­—")
    logger.info(f"ç™»å ´äººç‰©è¦ç´„: {len(character_summary)} æ–‡å­—")

    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—
    warmup_prompt_cache(story_text, character_summary)

    # å…¨ãƒ†ã‚¹ãƒˆçµæœã‚’ä¿å­˜
    all_results = []

    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œï¼ˆå…¨ã¦ã®çµ„ã¿åˆã‚ã›ã‚’ãƒ†ã‚¹ãƒˆï¼‰
    # å®Ÿé¨“ã®è¦æ¨¡ã‚’è€ƒæ…®ã—ã¦ã€ä»£è¡¨çš„ãªçµ„ã¿åˆã‚ã›ã®ã¿ãƒ†ã‚¹ãƒˆ
    test_combinations = [
        # GPT-5.1 ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³
        ("gpt-5.1", "gpt-4.1"),
        # GPT-5-mini
        ("gpt-5-mini", "gpt-5-mini"),
        # GPT-4 ç³»
        ("gpt-4.1", "gpt-4.1"),
        ("gpt-4o", "gpt-4o"),
        # Miniç³»
        ("gpt-4o-mini", "gpt-4o-mini"),
        # æ··åˆ
        ("gpt-5.1", "gpt-5-mini"),
        ("gpt-4.1", "gpt-5-mini"),
    ]

    total_tests = len(TEST_QUESTIONS) * len(test_combinations)
    current_test = 0

    # é–‹å§‹æ™‚åˆ»ã‚’è¨˜éŒ²
    start_time = time.time()

    for question_data in TEST_QUESTIONS:
        for mermaid_model, answer_model in test_combinations:
            current_test += 1

            # é€²æ—çŠ¶æ³ã‚’è¨ˆç®—
            progress_pct = (current_test / total_tests) * 100
            elapsed = time.time() - start_time
            if current_test > 1:
                avg_time = elapsed / (current_test - 1)
                remaining = avg_time * (total_tests - current_test)
                eta_str = f"æ®‹ã‚Šç´„{remaining/60:.1f}åˆ†"
            else:
                eta_str = "è¨ˆç®—ä¸­..."

            logger.info(f"\n{'='*80}")
            logger.info(f"ğŸ“Š é€²æ—: {current_test}/{total_tests} ({progress_pct:.1f}%) | çµŒéæ™‚é–“: {elapsed/60:.1f}åˆ† | {eta_str}")
            logger.info(f"{'='*80}")

            try:
                result = run_single_test(
                    question_data=question_data,
                    mermaid_model=mermaid_model,
                    answer_model=answer_model,
                    story_text=story_text,
                    character_summary=character_summary,
                    test_num=current_test,
                    total_tests=total_tests
                )
                all_results.append(result)

                # ä¸­é–“çµæœã‚’ä¿å­˜ï¼ˆ10ãƒ†ã‚¹ãƒˆã”ã¨ï¼‰
                if current_test % 10 == 0 or current_test == total_tests:
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    intermediate_file = f"model_comparison_intermediate_{timestamp}.json"
                    with open(intermediate_file, 'w', encoding='utf-8') as f:
                        json.dump(all_results, f, ensure_ascii=False, indent=2)
                    logger.info(f"ğŸ’¾ ä¸­é–“çµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: {intermediate_file}")

                # å°‘ã—å¾…æ©Ÿï¼ˆAPI rate limitå¯¾ç­–ï¼‰
                time.sleep(1)

            except Exception as e:
                logger.error(f"âŒ ãƒ†ã‚¹ãƒˆå¤±æ•—: {question_data['id']}, {mermaid_model}, {answer_model}: {str(e)}")
                import traceback
                logger.error(traceback.format_exc())
                logger.error("ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸãŸã‚ã€ãƒ†ã‚¹ãƒˆã‚’ä¸­æ­¢ã—ã¾ã™")
                raise

    # æœ€çµ‚çµæœã®ã‚µãƒãƒªãƒ¼
    total_elapsed = time.time() - start_time
    successful_tests = len(all_results)
    logger.info(f"\n{'='*80}")
    logger.info(f"âœ… å…¨ãƒ†ã‚¹ãƒˆå®Œäº†!")
    logger.info(f"æˆåŠŸ: {successful_tests}/{total_tests} ãƒ†ã‚¹ãƒˆ")
    logger.info(f"ç·å®Ÿè¡Œæ™‚é–“: {total_elapsed/60:.1f}åˆ†")
    logger.info(f"{'='*80}\n")

    return all_results


def save_results_to_csv(results: List[Dict], output_file: str = "model_comparison_results.csv"):
    """
    çµæœã‚’CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    """
    if not results:
        logger.warning("ä¿å­˜ã™ã‚‹çµæœãŒã‚ã‚Šã¾ã›ã‚“")
        return

    # CSVå‡ºåŠ›
    with open(output_file, 'w', newline='', encoding='utf-8-sig') as f:
        writer = csv.writer(f)

        # ãƒ˜ãƒƒãƒ€ãƒ¼
        headers = [
            "è³ªå•ID", "è³ªå•", "è³ªå•ã‚¿ã‚¤ãƒ—",
            "Mermaid/CSVãƒ¢ãƒ‡ãƒ«", "å›ç­”ãƒ¢ãƒ‡ãƒ«",
            "ç™»å ´äººç‰©åˆ¤å®š(s)", "ç™»å ´äººç‰©åˆ¤å®š(tokens)",
            "ä¸­å¿ƒäººç‰©ç‰¹å®š(s)", "ä¸­å¿ƒäººç‰©ç‰¹å®š(tokens)",
            "Mermaidç”Ÿæˆ(s)", "Mermaidç”Ÿæˆ(tokens)",
            "CSVå¤‰æ›(s)", "CSVå¤‰æ›(tokens)",
            "å›ç­”ç”Ÿæˆ(s)", "å›ç­”ç”Ÿæˆ(tokens)",
            "åˆè¨ˆæ™‚é–“(s)", "åˆè¨ˆãƒˆãƒ¼ã‚¯ãƒ³æ•°",
            "ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—"
        ]
        writer.writerow(headers)

        # ãƒ‡ãƒ¼ã‚¿è¡Œ
        for r in results:
            total_tokens = sum(p["tokens"]["total"] for p in r["processes"].values())

            row = [
                r["question_id"],
                r["question"],
                r["question_type"],
                r["mermaid_model"],
                r["answer_model"],
                f"{r['processes']['character_judgment']['time']:.2f}",
                r['processes']['character_judgment']['tokens']['total'],
                f"{r['processes']['center_person']['time']:.2f}",
                r['processes']['center_person']['tokens']['total'],
                f"{r['processes']['mermaid_generation']['time']:.2f}",
                r['processes']['mermaid_generation']['tokens']['total'],
                f"{r['processes']['csv_conversion']['time']:.2f}",
                r['processes']['csv_conversion']['tokens']['total'],
                f"{r['processes']['answer_generation']['time']:.2f}",
                r['processes']['answer_generation']['tokens']['total'],
                f"{r['total_time']:.2f}",
                total_tokens,
                r["timestamp"]
            ]
            writer.writerow(row)

    logger.info(f"âœ“ çµæœã‚’CSVã«ä¿å­˜ã—ã¾ã—ãŸ: {output_file}")


def save_detailed_results_to_json(results: List[Dict], output_file: str = "model_comparison_detailed.json"):
    """
    è©³ç´°ãªçµæœï¼ˆç”Ÿæˆå†…å®¹å«ã‚€ï¼‰ã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    """
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)

    logger.info(f"âœ“ è©³ç´°çµæœã‚’JSONã«ä¿å­˜ã—ã¾ã—ãŸ: {output_file}")


# =================================================
#           ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ
# =================================================

if __name__ == "__main__":
    logger.info("ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒå®Ÿé¨“ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’é–‹å§‹ã—ã¾ã™")
    logger.info(f"ãƒ†ã‚¹ãƒˆè³ªå•æ•°: {len(TEST_QUESTIONS)}")
    logger.info(f"Mermaid/CSVãƒ†ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«: {TEST_MODELS['mermaid_csv']}")
    logger.info(f"å›ç­”ç”Ÿæˆãƒ†ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«: {TEST_MODELS['answer_generation']}")

    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    results = run_all_tests()

    # çµæœä¿å­˜
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    csv_file = f"model_comparison_results_{timestamp}.csv"
    json_file = f"model_comparison_detailed_{timestamp}.json"

    save_results_to_csv(results, csv_file)
    save_detailed_results_to_json(results, json_file)

    logger.info("=" * 80)
    logger.info("å…¨ãƒ†ã‚¹ãƒˆå®Œäº†!")
    logger.info(f"ç·ãƒ†ã‚¹ãƒˆæ•°: {len(results)}")
    logger.info(f"CSVå‡ºåŠ›: {csv_file}")
    logger.info(f"JSONå‡ºåŠ›: {json_file}")
    logger.info("=" * 80)
