"""
å¿˜å´ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãƒ—ãƒ­ã‚°ãƒ©ãƒ 

ä¹…ã—ã¶ã‚Šã«èª­æ›¸ã‚’å†é–‹ã™ã‚‹æƒ³å®šã§ã€å¿˜å´ã‚’å«ã‚“ã è¦ç´„ã‚’ç”Ÿæˆã—ã¾ã™ã€‚
- è¤‡æ•°ã®æ–‡å­—æ•°ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆ2000, 2500æ–‡å­—ï¼‰
- å„ãƒ‘ã‚¿ãƒ¼ãƒ³ã§3ã¤ã®ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³
- shadow_text.json ã®ã¿ã«å¯¾å¿œ
"""

import json
import os
from pathlib import Path
from dotenv import load_dotenv
from openai import OpenAI

# .envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã¿ï¼ˆæ—¢å­˜ã®ç’°å¢ƒå¤‰æ•°ã‚’ä¸Šæ›¸ãï¼‰
load_dotenv(override=True)

# OpenAI APIã‚­ãƒ¼è¨­å®š
client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))

def load_novel(novel_file: str, max_chapters: int = 30) -> str:
    """
    å°èª¬ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€æŒ‡å®šç« æ•°ã¾ã§ã®æœ¬æ–‡ã‚’çµåˆ

    Args:
        novel_file: å°èª¬ãƒ•ã‚¡ã‚¤ãƒ«å
        max_chapters: èª­ã¿è¾¼ã‚€æœ€å¤§ç« æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 30ï¼‰

    Returns:
        çµåˆã•ã‚ŒãŸæœ¬æ–‡
    """
    with open(novel_file, "r", encoding="utf-8") as f:
        data = json.load(f)

    # æŒ‡å®šç« æ•°ã¾ã§å–å¾—
    chapters = data[:max_chapters]

    # æœ¬æ–‡ã‚’çµåˆ
    full_text = "\n\n".join([
        f"ã€{ch['section']}ç« ã€‘ {ch['title']}\n\n{ch['text']}"
        for ch in chapters
    ])

    return full_text


def generate_forgetting_text(novel_text: str, char_limit: int, model: str = "gpt-5.1") -> str:
    """
    å¿˜å´ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆ

    Args:
        novel_text: å°èª¬ã®æœ¬æ–‡
        char_limit: æ–‡å­—æ•°åˆ¶é™
        model: ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: gpt-5.1ï¼‰

    Returns:
        ç”Ÿæˆã•ã‚ŒãŸå¿˜å´ãƒ†ã‚­ã‚¹ãƒˆ
    """
    input_text = f"""ã“ã®å°èª¬ã‚’ä»¥ä¸‹ã®å½¢ã§è¦ç´„ã—ã¦ãã ã•ã„ã€‚

ã€è¦ç´„ã®æ¡ä»¶ã€‘
ãƒ»ä¹…ã—ã¶ã‚Šã«èª­ã‚“ã æƒ³å®š
ãƒ»ä¸»äººå…¬ã®å­˜åœ¨ã¯è¦šãˆã¦ã„ã‚‹
ãƒ»ä¸»äººå…¬å‘¨è¾ºã®äººç‰©ã‚‚ãªã‚“ã¨ãªãè¦šãˆã¦ã„ã‚‹
ãƒ»ãŸã è©³ç´°ã«ä½•ãŒã‚ã£ãŸã®ã‹ã¯è¦šãˆã¦ã„ãªã„
ãƒ»å‹˜é•ã„ã‚„æ··ä¹±ãƒ»æ¬ è½ãŒæ™‚ã€…å­˜åœ¨ã™ã‚‹
ãƒ»ç‰©èªã®å†…å®¹ã‚’è¿½ã†å½¢ã§ã‚ã‚‰ã™ã˜ã‚’ä½œæˆã™ã‚‹
ãƒ»å¿˜å´ã—ã¦ã„ã‚‹é¢¨ã®ãƒ­ãƒ¼ãƒ«ãƒ—ãƒ¬ã‚¤ã®ã‚ˆã†ãªæ–‡ç« ã¯ä¸è¦
ãƒ»ã‚ã‚‰ã™ã˜å½¢å¼ã§å‡ºåŠ›ã™ã‚‹ï¼ˆç®‡æ¡æ›¸ãã«ã—ãªã„ï¼‰

ã€é‡è¦ï¼šæ–‡å­—æ•°ã®æŒ‡å®šã€‘
ãƒ»å¿…ãš{char_limit}æ–‡å­—å‰å¾Œï¼ˆÂ±200æ–‡å­—ç¨‹åº¦ï¼‰ã§å‡ºåŠ›ã—ã¦ãã ã•ã„
ãƒ»æœ€ä½ã§ã‚‚{char_limit - 500}æ–‡å­—ä»¥ä¸Šã¯å¿…é ˆã§ã™
ãƒ»è¦šãˆã¦ã„ã‚‹ç¯„å›²ã§ã€ã§ãã‚‹ã ã‘è©³ã—ãå„ç« ã‚„ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®å†…å®¹ã‚’æ›¸ã„ã¦ãã ã•ã„
ãƒ»çŸ­ãã¾ã¨ã‚ã™ããªã„ã‚ˆã†ã«æ³¨æ„ã—ã¦ãã ã•ã„

ã€å°èª¬æœ¬æ–‡ã€‘
{novel_text}

ã€å¿˜å´ã‚’å«ã‚“ã è¦ç´„ã€‘
"""

    result = client.responses.create(
        model=model,
        input=input_text,
        reasoning={"effort": "medium"},
        text={"verbosity": "high"},
    )

    return result.output_text.strip()


def save_forgetting_text(novel_name: str, char_limit: int, pattern_num: int, text: str):
    """
    å¿˜å´ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜

    Args:
        novel_name: å°èª¬åï¼ˆ"beast" or "shadow"ï¼‰
        char_limit: æ–‡å­—æ•°åˆ¶é™
        pattern_num: ãƒ‘ã‚¿ãƒ¼ãƒ³ç•ªå·ï¼ˆ1-3ï¼‰
        text: ä¿å­˜ã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆ
    """
    output_dir = Path("forgetting_texts") / novel_name
    output_dir.mkdir(parents=True, exist_ok=True)

    filename = output_dir / f"{char_limit}chars_pattern{pattern_num}.txt"

    with open(filename, "w", encoding="utf-8") as f:
        f.write(text)

    print(f"âœ… ä¿å­˜å®Œäº†: {filename} ({len(text)}æ–‡å­—)")


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    # è¨­å®š: å„å°èª¬ã®èª­è€…ãŒèª­ã‚€æ¨å¥¨ç« ã®ä¸€ã¤å‰ã¾ã§ã‚’å¯¾è±¡ã¨ã™ã‚‹
    novels = {
        "shadow": {
            "file": "shadow_text.json",
            "max_chapters": 30  # 31-32ç« ã‚’èª­ã‚€ã®ã§ã€30ç« ã¾ã§ã®ã‚ã‚‰ã™ã˜
        },
        "sangoku_2": {
            "file": "sangoku_2_text.json",
            "max_chapters": 56  # 57-58ç« ã‚’èª­ã‚€ã®ã§ã€56ç« ã¾ã§ã®ã‚ã‚‰ã™ã˜
        },
        "ranpo": {
            "file": "ranpo_text_ruby.json",
            "max_chapters": 10  # 11-12ç« ã‚’èª­ã‚€ã®ã§ã€10ç« ã¾ã§ã®ã‚ã‚‰ã™ã˜
        },
        "texhnical_area": {
            "file": "texhnical_area_text.json",
            "max_chapters": 43  # 44-45ç« ã‚’èª­ã‚€ã®ã§ã€43ç« ã¾ã§ã®ã‚ã‚‰ã™ã˜
        },
        "online_utyu": {
            "file": "online_utyu_text.json",
            "max_chapters": 22  # 23-24ç« ã‚’èª­ã‚€ã®ã§ã€22ç« ã¾ã§ã®ã‚ã‚‰ã™ã˜
        }
    }

    char_limits = [500]  # 500æ–‡å­—ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ã¿ç”Ÿæˆ
    patterns_per_limit = 1  # 1ã¤ã®ã¿ç”Ÿæˆ
    model = "gpt-5.1"  # GPT-5.1ã‚’ä½¿ç”¨

    print("ğŸš€ å¿˜å´ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚’é–‹å§‹ã—ã¾ã™")
    print(f"ãƒ¢ãƒ‡ãƒ«: {model}")
    print(f"æ–‡å­—æ•°ãƒ‘ã‚¿ãƒ¼ãƒ³: {char_limits}")
    print(f"å„ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ç”Ÿæˆæ•°: {patterns_per_limit}")
    print()

    for novel_name, novel_config in novels.items():
        print(f"ğŸ“– {novel_name.upper()} ã®å‡¦ç†ã‚’é–‹å§‹")

        # å°èª¬æœ¬æ–‡ã‚’èª­ã¿è¾¼ã¿ï¼ˆå„å°èª¬ã®æ¨å¥¨ç« ã®ä¸€ã¤å‰ã¾ã§ï¼‰
        novel_text = load_novel(novel_config["file"], max_chapters=novel_config["max_chapters"])
        print(f"  æœ¬æ–‡èª­ã¿è¾¼ã¿å®Œäº†: {novel_config['max_chapters']}ç« ã¾ã§, {len(novel_text):,}æ–‡å­—")

        for char_limit in char_limits:
            print(f"\n  ğŸ“ {char_limit}æ–‡å­—ãƒ‘ã‚¿ãƒ¼ãƒ³:")

            for pattern_num in range(1, patterns_per_limit + 1):
                print(f"    ãƒ‘ã‚¿ãƒ¼ãƒ³{pattern_num}ã‚’ç”Ÿæˆä¸­...", end=" ")

                try:
                    # å¿˜å´ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
                    forgetting_text = generate_forgetting_text(
                        novel_text=novel_text,
                        char_limit=char_limit,
                        model=model
                    )

                    # ä¿å­˜
                    save_forgetting_text(
                        novel_name=novel_name,
                        char_limit=char_limit,
                        pattern_num=pattern_num,
                        text=forgetting_text
                    )

                except Exception as e:
                    print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")

        print(f"\nâœ… {novel_name.upper()} ã®å‡¦ç†å®Œäº†\n")

    print("ğŸ‰ å…¨ã¦ã®å¿˜å´ãƒ†ã‚­ã‚¹ãƒˆç”ŸæˆãŒå®Œäº†ã—ã¾ã—ãŸ")
    print(f"ğŸ“ ä¿å­˜å…ˆ: forgetting_texts/")


if __name__ == "__main__":
    main()
