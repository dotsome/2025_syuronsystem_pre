#!/usr/bin/env python3
# ===============================================
#  ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒå®Ÿé¨“ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
# ===============================================
import os
import json
import time
import csv
import logging
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any
from dotenv import load_dotenv
import openai

# ç’°å¢ƒå¤‰æ•°èª­ã¿è¾¼ã¿
load_dotenv()
client = openai.OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.FileHandler(f'model_comparison_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# =================================================
#           ãƒ†ã‚¹ãƒˆè¨­å®š
# =================================================

# å›ºå®šãƒ¢ãƒ‡ãƒ«ï¼ˆå¿…ãšGPT-5.1ã‚’ä½¿ç”¨ï¼‰
FIXED_MODELS = {
    "character_judgment": "gpt-5.1",  # ç™»å ´äººç‰©åˆ¤å®š
    "center_person": "gpt-5.1"         # ä¸­å¿ƒäººç‰©ç‰¹å®š
}

# ãƒ†ã‚¹ãƒˆå¯¾è±¡ãƒ¢ãƒ‡ãƒ«
TEST_MODELS = {
    "mermaid_csv": [
        "gpt-5.1",
        "gpt-5-mini",
        "gpt-4-2025-08-07",
        "gpt-4.1",
        "gpt-4o",
        "gpt-4o-mini"
    ],
    "answer_generation": [
        "gpt-5-mini",
        "gpt-4-2025-08-07",
        "gpt-4.1",
        "gpt-4o",
        "gpt-4o-mini"
    ]
}

# ãƒ†ã‚¹ãƒˆç”¨ã®è³ªå•
TEST_QUESTIONS = [
    {
        "id": "Q1",
        "question": "ãƒŸãƒŠã£ã¦èª°ã ã£ã‘ï¼Ÿ",
        "type": "character_identification"
    },
    {
        "id": "Q2",
        "question": "ã‚¿ãƒ‹ã‚¢ã¨ã‚«ãƒŠãƒ‡ã®é–¢ä¿‚æ€§ã«ã¤ã„ã¦æ•™ãˆã¦",
        "type": "relationship"
    },
    {
        "id": "Q3",
        "question": "ãƒ¬ã‚¤ãƒ³ã¯ã‚¢ãƒªã‚ªã‚¹ã®ã“ã¨ãŒãªã‚“ã§å«Œã„ãªã®ï¼Ÿ",
        "type": "character_motivation"
    },
    {
        "id": "Q4",
        "question": "ã‚¿ãƒ‹ã‚¢ã¨ãƒªãƒ¼ãƒ³ã®é–¢ä¿‚æ€§ã«ã¤ã„ã¦æ•™ãˆã¦",
        "type": "relationship"
    }
]

# =================================================
#           ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°
# =================================================

def openai_chat_timed(model: str, messages: List[Dict], log_label: str = None, **kwargs) -> Dict[str, Any]:
    """
    OpenAI APIã‚’å‘¼ã³å‡ºã—ã€æ™‚é–“ã¨ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’è¨ˆæ¸¬

    Returns:
        dict: {
            "response": ChatCompletion,
            "time": float,
            "tokens": {"prompt": int, "completion": int, "total": int},
            "content": str
        }
    """
    start_time = time.time()

    try:
        response = client.chat.completions.create(
            model=model,
            messages=messages,
            **kwargs
        )
        elapsed = time.time() - start_time

        # ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨é‡
        usage = response.usage
        tokens = {
            "prompt": usage.prompt_tokens if usage else 0,
            "completion": usage.completion_tokens if usage else 0,
            "total": usage.total_tokens if usage else 0
        }

        content = response.choices[0].message.content if response.choices else ""

        log_msg = f"âœ“ {log_label or 'APIå‘¼ã³å‡ºã—'}: model={model}, time={elapsed:.2f}s, tokens={tokens['prompt']}â†’{tokens['completion']}"
        logger.info(log_msg)

        return {
            "response": response,
            "time": elapsed,
            "tokens": tokens,
            "content": content
        }

    except Exception as e:
        elapsed = time.time() - start_time
        logger.error(f"âœ— {log_label or 'APIå‘¼ã³å‡ºã—'}å¤±æ•—: model={model}, time={elapsed:.2f}s, error={str(e)}")
        return {
            "response": None,
            "time": elapsed,
            "tokens": {"prompt": 0, "completion": 0, "total": 0},
            "content": f"ERROR: {str(e)}",
            "error": str(e)
        }


def load_story_text() -> str:
    """
    å°èª¬æœ¬æ–‡ã‚’èª­ã¿è¾¼ã¿ï¼ˆ31ãƒšãƒ¼ã‚¸ã¾ã§ï¼‰

    zikken_11month_v7.pyã¨åŒã˜å½¢å¼ã§ã€31ãƒšãƒ¼ã‚¸ç›®ï¼ˆSTART_PAGE=30, å®Ÿéš›ã®ãƒšãƒ¼ã‚¸index=30ï¼‰ã¾ã§èª­ã¿è¾¼ã‚€
    """
    START_PAGE = 30  # zikken_11month_v7.pyã¨åŒã˜è¨­å®š
    story_file = Path(__file__).parent / "beast_text.json"

    if story_file.exists():
        with open(story_file, 'r', encoding='utf-8') as f:
            data = json.load(f)

        # zikken_11month_v7.pyã¨åŒã˜å½¢å¼ã§ãƒšãƒ¼ã‚¸ã‚’ä½œæˆ
        pages_all = [f"ã€{sec['section']}ç« ã€‘ {sec['title']}\n\n{sec['text']}"
                     for sec in data]

        # 31ãƒšãƒ¼ã‚¸ç›®ã¾ã§ï¼ˆindex 0-30 = 31ãƒšãƒ¼ã‚¸ï¼‰
        story_text = "\n\n".join(pages_all[:START_PAGE + 1])

        logger.info(f"å°èª¬æœ¬æ–‡ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {START_PAGE + 1}ãƒšãƒ¼ã‚¸, {len(story_text)}æ–‡å­—")
        return story_text
    else:
        logger.warning(f"å°èª¬ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {story_file}")
        return "ï¼ˆã‚µãƒ³ãƒ—ãƒ«ãƒ†ã‚­ã‚¹ãƒˆï¼‰ã“ã‚Œã¯å°èª¬ã®æœ¬æ–‡ã§ã™ã€‚"


def load_character_summary() -> str:
    """ç™»å ´äººç‰©è¦ç´„ã‚’èª­ã¿è¾¼ã¿"""
    summary_file = Path(__file__).parent / "character_summary.txt"
    if summary_file.exists():
        with open(summary_file, 'r', encoding='utf-8') as f:
            return f.read()
    else:
        logger.warning(f"ç™»å ´äººç‰©è¦ç´„ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {summary_file}")
        return ""


# =================================================
#           å„ãƒ—ãƒ­ã‚»ã‚¹ã®å®Ÿè¡Œé–¢æ•°
# =================================================

def process_character_judgment(story_text: str, question: str, character_summary: str) -> Dict[str, Any]:
    """
    ç™»å ´äººç‰©åˆ¤å®šãƒ—ãƒ­ã‚»ã‚¹ï¼ˆå›ºå®š: GPT-5.1ï¼‰
    """
    prompt = f"""ä»¥ä¸‹ã®æ–‡ç« ã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒä»Šã¾ã§èª­ã‚“ã å°èª¬æœ¬æ–‡ã§ã™ã€‚
----- æœ¬æ–‡ã“ã“ã‹ã‚‰ -----
{story_text}
----- æœ¬æ–‡ã“ã“ã¾ã§ -----

ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ä»¥ä¸‹ã®è³ªå•ãŒã‚ã‚Šã¾ã—ãŸã€‚
è³ªå•: {question}

ã“ã®è³ªå•ãŒã€Œç™»å ´äººç‰©ã«é–¢ã™ã‚‹è³ªå•ã€ã‹ã©ã†ã‹ã‚’åˆ¤å®šã—ã¦ãã ã•ã„ã€‚
- ç™»å ´äººç‰©ã®é–¢ä¿‚ã€è¡Œå‹•ã€æ€§æ ¼ã€èƒŒæ™¯ãªã©ã«é–¢ã™ã‚‹è³ªå•ãªã‚‰ã€Œã¯ã„ã€
- ãã‚Œä»¥å¤–ãªã‚‰ã€Œã„ã„ãˆã€

JSONå½¢å¼ã§å›ç­”ã—ã¦ãã ã•ã„:
{{"is_character_question": "ã¯ã„" or "ã„ã„ãˆ", "reason": "ç†ç”±"}}"""

    messages = [
        {"role": "system", "content": "ã‚ãªãŸã¯è³ªå•ã‚’åˆ†é¡ã™ã‚‹ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"},
        {"role": "user", "content": prompt}
    ]

    return openai_chat_timed(
        model=FIXED_MODELS["character_judgment"],
        messages=messages,
        log_label="ç™»å ´äººç‰©åˆ¤å®š",
        temperature=0.0
    )


def process_center_person(story_text: str, question: str, character_summary: str) -> Dict[str, Any]:
    """
    ä¸­å¿ƒäººç‰©ç‰¹å®šãƒ—ãƒ­ã‚»ã‚¹ï¼ˆå›ºå®š: GPT-5.1ï¼‰
    """
    prompt = f"""ä»¥ä¸‹ã®æ–‡ç« ã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒä»Šã¾ã§èª­ã‚“ã å°èª¬æœ¬æ–‡ã§ã™ã€‚
----- æœ¬æ–‡ã“ã“ã‹ã‚‰ -----
{story_text}
----- æœ¬æ–‡ã“ã“ã¾ã§ -----

ä»¥ä¸‹ã¯ç™»å ´äººç‰©ã®è¦ç´„ã§ã™:
{character_summary}

ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•: {question}

ã“ã®è³ªå•ã«ç­”ãˆã‚‹ä¸Šã§ä¸­å¿ƒã¨ãªã‚‹äººç‰©ã‚’ç‰¹å®šã—ã¦ãã ã•ã„ã€‚
JSONå½¢å¼ã§å›ç­”:
{{"center_person": "äººç‰©å", "reason": "ç†ç”±"}}"""

    messages = [
        {"role": "system", "content": "ã‚ãªãŸã¯ç™»å ´äººç‰©ã‚’åˆ†æã™ã‚‹ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"},
        {"role": "user", "content": prompt}
    ]

    return openai_chat_timed(
        model=FIXED_MODELS["center_person"],
        messages=messages,
        log_label="ä¸­å¿ƒäººç‰©ç‰¹å®š",
        temperature=0.0
    )


def process_mermaid_generation(model: str, story_text: str, question: str,
                                center_person: str, character_summary: str) -> Dict[str, Any]:
    """
    Mermaidå›³ç”Ÿæˆãƒ—ãƒ­ã‚»ã‚¹ï¼ˆãƒ©ãƒ•ç”Ÿæˆï¼‰
    """
    prompt = f"""ä»¥ä¸‹ã®æ–‡ç« ã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒä»Šã¾ã§èª­ã‚“ã å°èª¬æœ¬æ–‡ã§ã™ã€‚
----- æœ¬æ–‡ã“ã“ã‹ã‚‰ -----
{story_text}
----- æœ¬æ–‡ã“ã“ã¾ã§ -----

ä»¥ä¸‹ã¯ç™»å ´äººç‰©ã®è¦ç´„ã§ã™:
{character_summary}

ä¸­å¿ƒäººç‰©: {center_person}
è³ªå•: {question}

ã“ã®è³ªå•ã«ç­”ãˆã‚‹ãŸã‚ã®äººç‰©é–¢ä¿‚å›³ã‚’Mermaidè¨˜æ³•ã§ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
ä¸­å¿ƒäººç‰©ã‚’ä¸­å¿ƒã«ã€é–¢é€£ã™ã‚‹äººç‰©ã¨ã®é–¢ä¿‚ã‚’å›³ç¤ºã—ã¦ãã ã•ã„ã€‚

ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ:
```mermaid
graph TD
  ...
```"""

    messages = [
        {"role": "system", "content": "ã‚ãªãŸã¯Mermaidå›³ã‚’ç”Ÿæˆã™ã‚‹ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"},
        {"role": "user", "content": prompt}
    ]

    # gpt-5-miniã¯temperatureã‚’ã‚µãƒãƒ¼ãƒˆã—ãªã„ãŸã‚ã€ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã£ã¦åˆ†å²
    kwargs = {"log_label": f"Mermaidç”Ÿæˆ({model})"}
    if "gpt-5-mini" not in model:
        kwargs["temperature"] = 0.3

    return openai_chat_timed(
        model=model,
        messages=messages,
        **kwargs
    )


def process_csv_conversion(model: str, mermaid_code: str) -> Dict[str, Any]:
    """
    CSVå¤‰æ›ãƒ—ãƒ­ã‚»ã‚¹
    """
    prompt = f"""ä»¥ä¸‹ã®Mermaidã‚³ãƒ¼ãƒ‰ã‚’CSVå½¢å¼ã«å¤‰æ›ã—ã¦ãã ã•ã„ã€‚

{mermaid_code}

CSVå½¢å¼:
from,to,label"""

    messages = [
        {"role": "system", "content": "ã‚ãªãŸã¯Mermaidã‚³ãƒ¼ãƒ‰ã‚’CSVã«å¤‰æ›ã™ã‚‹ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"},
        {"role": "user", "content": prompt}
    ]

    # gpt-5-miniã¯temperatureã‚’ã‚µãƒãƒ¼ãƒˆã—ãªã„ãŸã‚ã€ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã£ã¦åˆ†å²
    kwargs = {"log_label": f"CSVå¤‰æ›({model})"}
    if "gpt-5-mini" not in model:
        kwargs["temperature"] = 0.0

    return openai_chat_timed(
        model=model,
        messages=messages,
        **kwargs
    )


def process_answer_generation(model: str, story_text: str, question: str) -> Dict[str, Any]:
    """
    è³ªå•ã¸ã®å›ç­”ç”Ÿæˆãƒ—ãƒ­ã‚»ã‚¹
    """
    prompt = f"""ä»¥ä¸‹ã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒã“ã‚Œã¾ã§ã«èª­ã‚“ã å°èª¬æœ¬æ–‡ã§ã™ã€‚

----- æœ¬æ–‡ã“ã“ã‹ã‚‰ -----
{story_text}
----- æœ¬æ–‡ã“ã“ã¾ã§ -----

# æŒ‡ç¤º
ã“ã®æœ¬æ–‡ã®å†…å®¹ã‚’æ ¹æ‹ ã«ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«æ—¥æœ¬èªã§ä¸å¯§ã«ç­”ãˆã¦ãã ã•ã„ã€‚

è³ªå•: {question}"""

    messages = [
        {"role": "system", "content": "ã‚ãªãŸã¯èª­ã‚“ã§ã„ã‚‹å°èª¬ã«ã¤ã„ã¦è³ªå•ã«ç­”ãˆã‚‹ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"},
        {"role": "user", "content": prompt}
    ]

    # gpt-5-miniã¯temperatureã‚’ã‚µãƒãƒ¼ãƒˆã—ãªã„ãŸã‚ã€ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã£ã¦åˆ†å²
    kwargs = {"log_label": f"å›ç­”ç”Ÿæˆ({model})"}
    if "gpt-5-mini" not in model:
        kwargs["temperature"] = 0.7

    return openai_chat_timed(
        model=model,
        messages=messages,
        **kwargs
    )


# =================================================
#           ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
# =================================================

def run_single_test(question_data: Dict, mermaid_model: str, answer_model: str,
                    story_text: str, character_summary: str, test_num: int = 0, total_tests: int = 0) -> Dict[str, Any]:
    """
    1ã¤ã®ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã‚’å®Ÿè¡Œ
    """
    question_id = question_data["id"]
    question = question_data["question"]

    progress_info = f"[{test_num}/{total_tests}]" if total_tests > 0 else ""
    logger.info(f"\n{'='*60}")
    logger.info(f"{progress_info} ãƒ†ã‚¹ãƒˆé–‹å§‹: {question_id} - Mermaid/CSV={mermaid_model}, Answer={answer_model}")
    logger.info(f"è³ªå•: {question}")
    logger.info(f"{'='*60}")

    results = {
        "question_id": question_id,
        "question": question,
        "question_type": question_data["type"],
        "mermaid_model": mermaid_model,
        "answer_model": answer_model,
        "processes": {},
        "outputs": {},
        "total_time": 0,
        "timestamp": datetime.now().isoformat()
    }

    # 1. ç™»å ´äººç‰©åˆ¤å®šï¼ˆå›ºå®š: GPT-5.1ï¼‰
    judgment_result = process_character_judgment(story_text, question, character_summary)
    results["processes"]["character_judgment"] = {
        "model": FIXED_MODELS["character_judgment"],
        "time": judgment_result["time"],
        "tokens": judgment_result["tokens"]
    }
    results["outputs"]["character_judgment"] = judgment_result["content"]

    # 2. ä¸­å¿ƒäººç‰©ç‰¹å®šï¼ˆå›ºå®š: GPT-5.1ï¼‰
    center_result = process_center_person(story_text, question, character_summary)
    results["processes"]["center_person"] = {
        "model": FIXED_MODELS["center_person"],
        "time": center_result["time"],
        "tokens": center_result["tokens"]
    }
    results["outputs"]["center_person"] = center_result["content"]

    # ä¸­å¿ƒäººç‰©ã‚’æŠ½å‡ºï¼ˆç°¡æ˜“ç‰ˆï¼‰
    try:
        center_data = json.loads(center_result["content"])
        center_person = center_data.get("center_person", "ä¸æ˜")
    except:
        center_person = "ä¸æ˜"

    # 3. Mermaidç”Ÿæˆ
    mermaid_result = process_mermaid_generation(
        mermaid_model, story_text, question, center_person, character_summary
    )
    results["processes"]["mermaid_generation"] = {
        "model": mermaid_model,
        "time": mermaid_result["time"],
        "tokens": mermaid_result["tokens"]
    }
    results["outputs"]["mermaid_code"] = mermaid_result["content"]

    # 4. CSVå¤‰æ›
    csv_result = process_csv_conversion(mermaid_model, mermaid_result["content"])
    results["processes"]["csv_conversion"] = {
        "model": mermaid_model,
        "time": csv_result["time"],
        "tokens": csv_result["tokens"]
    }
    results["outputs"]["csv_data"] = csv_result["content"]

    # 5. å›ç­”ç”Ÿæˆ
    answer_result = process_answer_generation(answer_model, story_text, question)
    results["processes"]["answer_generation"] = {
        "model": answer_model,
        "time": answer_result["time"],
        "tokens": answer_result["tokens"]
    }
    results["outputs"]["answer"] = answer_result["content"]

    # åˆè¨ˆæ™‚é–“
    results["total_time"] = sum(p["time"] for p in results["processes"].values())

    # ãƒ—ãƒ­ã‚»ã‚¹åˆ¥ã®æ™‚é–“å†…è¨³ã‚’è¡¨ç¤º
    process_times = {
        "ç™»å ´äººç‰©åˆ¤å®š": results["processes"]["character_judgment"]["time"],
        "ä¸­å¿ƒäººç‰©ç‰¹å®š": results["processes"]["center_person"]["time"],
        "Mermaidç”Ÿæˆ": results["processes"]["mermaid_generation"]["time"],
        "CSVå¤‰æ›": results["processes"]["csv_conversion"]["time"],
        "å›ç­”ç”Ÿæˆ": results["processes"]["answer_generation"]["time"]
    }

    logger.info(f"\n{'='*60}")
    logger.info(f"{progress_info} ãƒ†ã‚¹ãƒˆå®Œäº†: {question_id}")
    logger.info(f"åˆè¨ˆæ™‚é–“: {results['total_time']:.2f}s")
    logger.info(f"å†…è¨³: " + " | ".join([f"{k}={v:.2f}s" for k, v in process_times.items()]))
    logger.info(f"{'='*60}\n")

    return results


def warmup_prompt_cache(story_text: str, character_summary: str):
    """
    ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ï¼ˆzikken_11month_v7.pyã¨åŒã˜å‡¦ç†ï¼‰

    ã“ã‚Œã«ã‚ˆã‚Šã€æœ€åˆã®ãƒ†ã‚¹ãƒˆã‹ã‚‰é«˜é€Ÿãªå¿œç­”ãŒå¯èƒ½ã«ãªã‚‹
    """
    logger.info("\n" + "=" * 80)
    logger.info("ğŸ”¥ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ä¸­...")
    logger.info("=" * 80)

    try:
        # 1. æœ¬æ–‡ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä½œæˆ
        warmup_prompt_story = f"""
æœ¬æ–‡:
{story_text}

è³ªå•: ä¸»äººå…¬ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„

è¦ä»¶:
- graph LR ã¾ãŸã¯ graph TD ã§é–‹å§‹
- **ä¸»äººå…¬ã‚’ä¸­å¿ƒ**ã«ã€ç›´æ¥é–¢ã‚ã‚‹ä¸»è¦äººç‰©ã®ã¿ã‚’å«ã‚ã‚‹
- ç™»å ´äººç‰©ã¯ç‰©èªä¸Šé‡è¦ãªäººç‰©ã«é™å®šã™ã‚‹ï¼ˆ5-10äººç¨‹åº¦ï¼‰
- é–¢ä¿‚æ€§ã®è¡¨ç¾ï¼š
  * åŒæ–¹å‘ã®é–¢ä¿‚: <--> ã‚’ä½¿ç”¨ï¼ˆä¾‹: å‹äººã€ä»²é–“ã€æ‹äººãªã©ï¼‰
  * ä¸€æ–¹å‘ã®é–¢ä¿‚: --> ã‚’ä½¿ç”¨ï¼ˆä¾‹: ä¸Šå¸â†’éƒ¨ä¸‹ã€å¸«åŒ â†’å¼Ÿå­ãªã©ï¼‰
  * ç‚¹ç·šçŸ¢å° -.-> ã¯è£œåŠ©çš„ãªé–¢ä¿‚ã«ä½¿ç”¨
- **é‡è¦**: åŒã˜2äººã®é–“ã®é–¢ä¿‚ã¯æœ€å¤§2æœ¬ã¾ã§ï¼ˆAã‹ã‚‰Bã€Bã‹ã‚‰Aï¼‰
- ã‚¨ãƒƒã‚¸ã«ã¯ç°¡æ½”ãªæ—¥æœ¬èªãƒ©ãƒ™ãƒ«ã‚’ä»˜ã‘ã‚‹ï¼ˆ5æ–‡å­—ä»¥å†…æ¨å¥¨ï¼‰
- å¿…è¦ã«å¿œã˜ã¦subgraphã§ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ï¼ˆä¾‹: å‹‡è€…ãƒ‘ãƒ¼ãƒ†ã‚£ãƒ¼ã€é­”ç‹è»ãªã©ï¼‰
- ä¸»äººå…¬ã«ç›´æ¥é–¢ã‚ã‚‰ãªã„äººç‰©é–“ã®é–¢ä¿‚ã¯çœç•¥ã™ã‚‹

ä»¥ä¸Šã®è³ªå•ã¨æœ¬æ–‡ã‚’åŸºã«ã€ã€Œä¸»äººå…¬ã€ã‚’ä¸­å¿ƒã¨ã—ãŸä¸»è¦ç™»å ´äººç‰©ã®é–¢ä¿‚å›³ã‚’Mermaidå½¢å¼ã§ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
å‡ºåŠ›ã¯Mermaidã‚³ãƒ¼ãƒ‰ã®ã¿ï¼ˆèª¬æ˜ä¸è¦ï¼‰
"""

        logger.info("æœ¬æ–‡ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä½œæˆä¸­...")
        _ = openai_chat_timed(
            "gpt-5.1",
            messages=[
                {"role": "system", "content": "Mermaidå›³ã‚’ç”Ÿæˆã™ã‚‹å°‚é–€å®¶ã§ã™ã€‚"},
                {"role": "user", "content": warmup_prompt_story}
            ],
            temperature=0.3,
            log_label="ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ï¼ˆæœ¬æ–‡ï¼‰"
        )

        # 2. ç™»å ´äººç‰©æƒ…å ±ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä½œæˆ
        if character_summary:
            warmup_prompt_character = f"""
ç™»å ´äººç‰©æƒ…å ±:
{character_summary}

---

è³ªå•: ä¸»äººå…¬ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„

ã“ã®è³ªå•ã®ä¸­å¿ƒã¨ãªã‚‹ç™»å ´äººç‰©ã®åå‰ã‚’1ã¤ã ã‘ç­”ãˆã¦ãã ã•ã„ã€‚

è¦ä»¶:
- ç™»å ´äººç‰©æƒ…å ±ã«è¨˜è¼‰ã•ã‚Œã¦ã„ã‚‹æ­£ç¢ºãªäººç‰©åã§å›ç­”
- äººç‰©åã®ã¿ã‚’1è¡Œã§å‡ºåŠ›ï¼ˆèª¬æ˜ä¸è¦ï¼‰

å›ç­”:
"""

            logger.info("ç™»å ´äººç‰©æƒ…å ±ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä½œæˆä¸­...")
            _ = openai_chat_timed(
                "gpt-5.1",
                messages=[
                    {"role": "system", "content": "è³ªå•ã®ä¸­å¿ƒäººç‰©ã‚’ç‰¹å®šã—ã¾ã™ã€‚"},
                    {"role": "user", "content": warmup_prompt_character}
                ],
                temperature=0,
                log_label="ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ï¼ˆç™»å ´äººç‰©ï¼‰"
            )

        logger.info("âœ… ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ãŒå®Œäº†ã—ã¾ã—ãŸ")
        logger.info("=" * 80 + "\n")

    except Exception as e:
        logger.warning(f"âš ï¸ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸãŒã€ãƒ†ã‚¹ãƒˆã‚’ç¶šè¡Œã—ã¾ã™: {e}")


def run_all_tests():
    """
    å…¨ã¦ã®ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹ã‚’å®Ÿè¡Œ
    """
    logger.info("=" * 80)
    logger.info("ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒå®Ÿé¨“ã‚’é–‹å§‹ã—ã¾ã™")
    logger.info("=" * 80)

    # å°èª¬æœ¬æ–‡ã¨ç™»å ´äººç‰©è¦ç´„ã‚’èª­ã¿è¾¼ã¿
    story_text = load_story_text()
    character_summary = load_character_summary()

    logger.info(f"å°èª¬æœ¬æ–‡: {len(story_text)} æ–‡å­—")
    logger.info(f"ç™»å ´äººç‰©è¦ç´„: {len(character_summary)} æ–‡å­—")

    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—
    warmup_prompt_cache(story_text, character_summary)

    # å…¨ãƒ†ã‚¹ãƒˆçµæœã‚’ä¿å­˜
    all_results = []

    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œï¼ˆå…¨ã¦ã®çµ„ã¿åˆã‚ã›ã‚’ãƒ†ã‚¹ãƒˆï¼‰
    # å®Ÿé¨“ã®è¦æ¨¡ã‚’è€ƒæ…®ã—ã¦ã€ä»£è¡¨çš„ãªçµ„ã¿åˆã‚ã›ã®ã¿ãƒ†ã‚¹ãƒˆ
    test_combinations = [
        # GPT-5.1 ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³
        ("gpt-5.1", "gpt-4.1"),
        # GPT-5-mini
        ("gpt-5-mini", "gpt-5-mini"),
        # GPT-4 ç³»
        ("gpt-4.1", "gpt-4.1"),
        ("gpt-4o", "gpt-4o"),
        # Miniç³»
        ("gpt-4o-mini", "gpt-4o-mini"),
        # æ··åˆ
        ("gpt-5.1", "gpt-5-mini"),
        ("gpt-4.1", "gpt-5-mini"),
    ]

    total_tests = len(TEST_QUESTIONS) * len(test_combinations)
    current_test = 0

    # é–‹å§‹æ™‚åˆ»ã‚’è¨˜éŒ²
    start_time = time.time()

    for question_data in TEST_QUESTIONS:
        for mermaid_model, answer_model in test_combinations:
            current_test += 1

            # é€²æ—çŠ¶æ³ã‚’è¨ˆç®—
            progress_pct = (current_test / total_tests) * 100
            elapsed = time.time() - start_time
            if current_test > 1:
                avg_time = elapsed / (current_test - 1)
                remaining = avg_time * (total_tests - current_test)
                eta_str = f"æ®‹ã‚Šç´„{remaining/60:.1f}åˆ†"
            else:
                eta_str = "è¨ˆç®—ä¸­..."

            logger.info(f"\n{'='*80}")
            logger.info(f"ğŸ“Š é€²æ—: {current_test}/{total_tests} ({progress_pct:.1f}%) | çµŒéæ™‚é–“: {elapsed/60:.1f}åˆ† | {eta_str}")
            logger.info(f"{'='*80}")

            try:
                result = run_single_test(
                    question_data=question_data,
                    mermaid_model=mermaid_model,
                    answer_model=answer_model,
                    story_text=story_text,
                    character_summary=character_summary,
                    test_num=current_test,
                    total_tests=total_tests
                )
                all_results.append(result)

                # ä¸­é–“çµæœã‚’ä¿å­˜ï¼ˆ10ãƒ†ã‚¹ãƒˆã”ã¨ï¼‰
                if current_test % 10 == 0 or current_test == total_tests:
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    intermediate_file = f"model_comparison_intermediate_{timestamp}.json"
                    with open(intermediate_file, 'w', encoding='utf-8') as f:
                        json.dump(all_results, f, ensure_ascii=False, indent=2)
                    logger.info(f"ğŸ’¾ ä¸­é–“çµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: {intermediate_file}")

                # å°‘ã—å¾…æ©Ÿï¼ˆAPI rate limitå¯¾ç­–ï¼‰
                time.sleep(1)

            except Exception as e:
                logger.error(f"âŒ ãƒ†ã‚¹ãƒˆå¤±æ•—: {question_data['id']}, {mermaid_model}, {answer_model}: {str(e)}")
                import traceback
                logger.error(traceback.format_exc())
                continue

    # æœ€çµ‚çµæœã®ã‚µãƒãƒªãƒ¼
    total_elapsed = time.time() - start_time
    successful_tests = len(all_results)
    logger.info(f"\n{'='*80}")
    logger.info(f"âœ… å…¨ãƒ†ã‚¹ãƒˆå®Œäº†!")
    logger.info(f"æˆåŠŸ: {successful_tests}/{total_tests} ãƒ†ã‚¹ãƒˆ")
    logger.info(f"ç·å®Ÿè¡Œæ™‚é–“: {total_elapsed/60:.1f}åˆ†")
    logger.info(f"{'='*80}\n")

    return all_results


def save_results_to_csv(results: List[Dict], output_file: str = "model_comparison_results.csv"):
    """
    çµæœã‚’CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    """
    if not results:
        logger.warning("ä¿å­˜ã™ã‚‹çµæœãŒã‚ã‚Šã¾ã›ã‚“")
        return

    # CSVå‡ºåŠ›
    with open(output_file, 'w', newline='', encoding='utf-8-sig') as f:
        writer = csv.writer(f)

        # ãƒ˜ãƒƒãƒ€ãƒ¼
        headers = [
            "è³ªå•ID", "è³ªå•", "è³ªå•ã‚¿ã‚¤ãƒ—",
            "Mermaid/CSVãƒ¢ãƒ‡ãƒ«", "å›ç­”ãƒ¢ãƒ‡ãƒ«",
            "ç™»å ´äººç‰©åˆ¤å®š(s)", "ç™»å ´äººç‰©åˆ¤å®š(tokens)",
            "ä¸­å¿ƒäººç‰©ç‰¹å®š(s)", "ä¸­å¿ƒäººç‰©ç‰¹å®š(tokens)",
            "Mermaidç”Ÿæˆ(s)", "Mermaidç”Ÿæˆ(tokens)",
            "CSVå¤‰æ›(s)", "CSVå¤‰æ›(tokens)",
            "å›ç­”ç”Ÿæˆ(s)", "å›ç­”ç”Ÿæˆ(tokens)",
            "åˆè¨ˆæ™‚é–“(s)", "åˆè¨ˆãƒˆãƒ¼ã‚¯ãƒ³æ•°",
            "ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—"
        ]
        writer.writerow(headers)

        # ãƒ‡ãƒ¼ã‚¿è¡Œ
        for r in results:
            total_tokens = sum(p["tokens"]["total"] for p in r["processes"].values())

            row = [
                r["question_id"],
                r["question"],
                r["question_type"],
                r["mermaid_model"],
                r["answer_model"],
                f"{r['processes']['character_judgment']['time']:.2f}",
                r['processes']['character_judgment']['tokens']['total'],
                f"{r['processes']['center_person']['time']:.2f}",
                r['processes']['center_person']['tokens']['total'],
                f"{r['processes']['mermaid_generation']['time']:.2f}",
                r['processes']['mermaid_generation']['tokens']['total'],
                f"{r['processes']['csv_conversion']['time']:.2f}",
                r['processes']['csv_conversion']['tokens']['total'],
                f"{r['processes']['answer_generation']['time']:.2f}",
                r['processes']['answer_generation']['tokens']['total'],
                f"{r['total_time']:.2f}",
                total_tokens,
                r["timestamp"]
            ]
            writer.writerow(row)

    logger.info(f"âœ“ çµæœã‚’CSVã«ä¿å­˜ã—ã¾ã—ãŸ: {output_file}")


def save_detailed_results_to_json(results: List[Dict], output_file: str = "model_comparison_detailed.json"):
    """
    è©³ç´°ãªçµæœï¼ˆç”Ÿæˆå†…å®¹å«ã‚€ï¼‰ã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    """
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)

    logger.info(f"âœ“ è©³ç´°çµæœã‚’JSONã«ä¿å­˜ã—ã¾ã—ãŸ: {output_file}")


# =================================================
#           ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ
# =================================================

if __name__ == "__main__":
    logger.info("ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒå®Ÿé¨“ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’é–‹å§‹ã—ã¾ã™")
    logger.info(f"ãƒ†ã‚¹ãƒˆè³ªå•æ•°: {len(TEST_QUESTIONS)}")
    logger.info(f"Mermaid/CSVãƒ†ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«: {TEST_MODELS['mermaid_csv']}")
    logger.info(f"å›ç­”ç”Ÿæˆãƒ†ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«: {TEST_MODELS['answer_generation']}")

    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    results = run_all_tests()

    # çµæœä¿å­˜
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    csv_file = f"model_comparison_results_{timestamp}.csv"
    json_file = f"model_comparison_detailed_{timestamp}.json"

    save_results_to_csv(results, csv_file)
    save_detailed_results_to_json(results, json_file)

    logger.info("=" * 80)
    logger.info("å…¨ãƒ†ã‚¹ãƒˆå®Œäº†!")
    logger.info(f"ç·ãƒ†ã‚¹ãƒˆæ•°: {len(results)}")
    logger.info(f"CSVå‡ºåŠ›: {csv_file}")
    logger.info(f"JSONå‡ºåŠ›: {json_file}")
    logger.info("=" * 80)
