#!/usr/bin/env python3
"""
Prompt CachingãŒå®Ÿéš›ã«åŠ¹ã„ã¦ã„ã‚‹ã‹ç¢ºèªã™ã‚‹ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""
import os
import time
from dotenv import load_dotenv
import openai
import json

load_dotenv()
client = openai.OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))

# é•·ã„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”¨æ„ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹æœã‚’ç¢ºèªã™ã‚‹ãŸã‚ï¼‰
with open("beast_text.json", "r", encoding="utf-8") as f:
    story_data = json.load(f)

story_text = "\n\n".join([
    f"ã€{sec['section']}ç« ã€‘ {sec['title']}\n\n{sec['text']}"
    for sec in story_data[:31]  # 31ãƒšãƒ¼ã‚¸
])

print("=" * 80)
print("Prompt Caching ãƒ†ã‚¹ãƒˆ")
print("=" * 80)
print(f"æœ¬æ–‡ã‚µã‚¤ã‚º: {len(story_text)} æ–‡å­—\n")

# ãƒ†ã‚¹ãƒˆã™ã‚‹ãƒ¢ãƒ‡ãƒ«
test_models = ["gpt-4o", "gpt-4o-mini", "gpt-5.1", "gpt-4.1"]

for model in test_models:
    print(f"\n{'=' * 80}")
    print(f"ãƒ¢ãƒ‡ãƒ«: {model}")
    print(f"{'=' * 80}")

    # åŒã˜ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§2å›å‘¼ã³å‡ºã—
    for attempt in [1, 2]:
        print(f"\n[{attempt}å›ç›®]")

        prompt = f"""æœ¬æ–‡:
{story_text}

è³ªå•: ãƒ¬ã‚¤ãƒ³ã£ã¦èª°ã§ã™ã‹ï¼Ÿ

ä¸Šè¨˜ã®è³ªå•ã«ç°¡æ½”ã«ç­”ãˆã¦ãã ã•ã„ã€‚"""

        try:
            start = time.time()
            response = client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": "è³ªå•ã«ç­”ãˆã‚‹ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3
            )
            elapsed = time.time() - start

            usage = response.usage

            print(f"  æ™‚é–“: {elapsed:.2f}s")
            print(f"  Prompt tokens: {usage.prompt_tokens}")
            print(f"  Completion tokens: {usage.completion_tokens}")
            print(f"  Total tokens: {usage.total_tokens}")

            # Prompt Cachingæƒ…å ±ã‚’ç¢ºèª
            if hasattr(usage, 'prompt_tokens_details'):
                details = usage.prompt_tokens_details
                if hasattr(details, 'cached_tokens'):
                    print(f"  âœ… Cached tokens: {details.cached_tokens}")
                else:
                    print(f"  â„¹ï¸  prompt_tokens_details ã«cached_tokensãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãªã—")
            else:
                print(f"  â„¹ï¸  usageã«prompt_tokens_detailsãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãªã—")

            # å°‘ã—å¾…ã¤
            if attempt == 1:
                time.sleep(2)

        except Exception as e:
            print(f"  âŒ ã‚¨ãƒ©ãƒ¼: {e}")
            break

print("\n" + "=" * 80)
print("ãƒ†ã‚¹ãƒˆå®Œäº†")
print("=" * 80)
print("\nğŸ’¡ çµæœã®è¦‹æ–¹:")
print("  - 2å›ç›®ã®cached_tokensãŒ0ã‚ˆã‚Šå¤§ãã„ â†’ Prompt Cachingæœ‰åŠ¹")
print("  - 2å›ç›®ã‚‚prompt_tokensãŒåŒã˜ â†’ Prompt Cachingç„¡åŠ¹")
print("  - prompt_tokens_detailsãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒãªã„ â†’ ãƒ¢ãƒ‡ãƒ«ãŒéå¯¾å¿œ")
